{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chapter 9 â€“ Up and running with TensorFlow**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_This notebook contains all the sample code and solutions to the exercises in chapter 9._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's make sure this notebook works well in both python 2 and 3, import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"tensorflow\"\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True):\n",
    "    path = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID, fig_id + \".png\")\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating and running a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/weiwu/.virtualenvs/nlp/local/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "x = tf.Variable(3, name=\"x\")\n",
    "y = tf.Variable(4, name=\"y\")\n",
    "f = x*x*y + y + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add_1:0' shape=() dtype=int32>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(x.initializer)\n",
    "sess.run(y.initializer)\n",
    "result = sess.run(f)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    x.initializer.run()\n",
    "    y.initializer.run()\n",
    "    result = f.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    result = f.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "init.run()\n",
    "result = f.eval()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Managing graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "x1 = tf.Variable(1)\n",
    "x1.graph is tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    x2 = tf.Variable(2)\n",
    "\n",
    "x2.graph is graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2.graph is tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "w = tf.constant(3)\n",
    "x = w + 2\n",
    "y = x + 5\n",
    "z = x * 3\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(y.eval())  # 10\n",
    "    print(z.eval())  # 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    y_val, z_val = sess.run([y, z])\n",
    "    print(y_val)  # 10\n",
    "    print(z_val)  # 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Normal Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "m, n = housing.data.shape\n",
    "housing_data_plus_bias = np.c_[np.ones((m, 1)), housing.data]\n",
    "\n",
    "X = tf.constant(housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "XT = tf.transpose(X)\n",
    "theta = tf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul(XT, X)), XT), y)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    theta_value = theta.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data', 'feature_names', 'DESCR', 'target']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   8.3252    ,   41.        ,    6.98412698, ...,    2.55555556,\n",
       "          37.88      , -122.23      ],\n",
       "       [   8.3014    ,   21.        ,    6.23813708, ...,    2.10984183,\n",
       "          37.86      , -122.22      ],\n",
       "       [   7.2574    ,   52.        ,    8.28813559, ...,    2.80225989,\n",
       "          37.85      , -122.24      ],\n",
       "       ...,\n",
       "       [   1.7       ,   17.        ,    5.20554273, ...,    2.3256351 ,\n",
       "          39.43      , -121.22      ],\n",
       "       [   1.8672    ,   18.        ,    5.32951289, ...,    2.12320917,\n",
       "          39.43      , -121.32      ],\n",
       "       [   2.3886    ,   16.        ,    5.25471698, ...,    2.61698113,\n",
       "          39.37      , -121.24      ]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   1.        ,    8.3252    ,   41.        , ...,    2.55555556,\n",
       "          37.88      , -122.23      ],\n",
       "       [   1.        ,    8.3014    ,   21.        , ...,    2.10984183,\n",
       "          37.86      , -122.22      ],\n",
       "       [   1.        ,    7.2574    ,   52.        , ...,    2.80225989,\n",
       "          37.85      , -122.24      ],\n",
       "       ...,\n",
       "       [   1.        ,    1.7       ,   17.        , ...,    2.3256351 ,\n",
       "          39.43      , -121.22      ],\n",
       "       [   1.        ,    1.8672    ,   18.        , ...,    2.12320917,\n",
       "          39.43      , -121.32      ],\n",
       "       [   1.        ,    2.3886    ,   16.        , ...,    2.61698113,\n",
       "          39.37      , -121.24      ]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_data_plus_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.7465141e+01],\n",
       "       [ 4.3573415e-01],\n",
       "       [ 9.3382923e-03],\n",
       "       [-1.0662201e-01],\n",
       "       [ 6.4410698e-01],\n",
       "       [-4.2513184e-06],\n",
       "       [-3.7732250e-03],\n",
       "       [-4.2664889e-01],\n",
       "       [-4.4051403e-01]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare with pure NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.69419202e+01]\n",
      " [ 4.36693293e-01]\n",
      " [ 9.43577803e-03]\n",
      " [-1.07322041e-01]\n",
      " [ 6.45065694e-01]\n",
      " [-3.97638942e-06]\n",
      " [-3.78654265e-03]\n",
      " [-4.21314378e-01]\n",
      " [-4.34513755e-01]]\n"
     ]
    }
   ],
   "source": [
    "X = housing_data_plus_bias\n",
    "y = housing.target.reshape(-1, 1)\n",
    "theta_numpy = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
    "\n",
    "print(theta_numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare with Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.69419202e+01]\n",
      " [ 4.36693293e-01]\n",
      " [ 9.43577803e-03]\n",
      " [-1.07322041e-01]\n",
      " [ 6.45065694e-01]\n",
      " [-3.97638942e-06]\n",
      " [-3.78654265e-03]\n",
      " [-4.21314378e-01]\n",
      " [-4.34513755e-01]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(housing.data, housing.target.reshape(-1, 1))\n",
    "\n",
    "print(np.r_[lin_reg.intercept_.reshape(-1, 1), lin_reg.coef_.T])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent requires scaling the feature vectors first. We could do this using TF, but let's just use Scikit-Learn for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaled_housing_data = scaler.fit_transform(housing.data)\n",
    "scaled_housing_data_plus_bias = np.c_[np.ones((m, 1)), scaled_housing_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.00000000e+00   6.60969987e-17   5.50808322e-18   6.60969987e-17\n",
      "  -1.06030602e-16  -1.10161664e-17   3.44255201e-18  -1.07958431e-15\n",
      "  -8.52651283e-15]\n",
      "[ 0.38915536  0.36424355  0.5116157  ..., -0.06612179 -0.06360587\n",
      "  0.01359031]\n",
      "0.111111111111\n",
      "(20640, 9)\n"
     ]
    }
   ],
   "source": [
    "print(scaled_housing_data_plus_bias.mean(axis=0))\n",
    "print(scaled_housing_data_plus_bias.mean(axis=1))\n",
    "print(scaled_housing_data_plus_bias.mean())\n",
    "print(scaled_housing_data_plus_bias.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually computing the gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 9.16154\n",
      "Epoch 100 MSE = 0.714501\n",
      "Epoch 200 MSE = 0.566705\n",
      "Epoch 300 MSE = 0.555572\n",
      "Epoch 400 MSE = 0.548812\n",
      "Epoch 500 MSE = 0.543636\n",
      "Epoch 600 MSE = 0.539629\n",
      "Epoch 700 MSE = 0.536509\n",
      "Epoch 800 MSE = 0.534068\n",
      "Epoch 900 MSE = 0.532147\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "gradients = 2/m * tf.matmul(tf.transpose(X), error)\n",
    "training_op = tf.assign(theta, theta - learning_rate * gradients)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval())\n",
    "        sess.run(training_op)\n",
    "    \n",
    "    best_theta = theta.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.06855249],\n",
       "       [ 0.88740271],\n",
       "       [ 0.14401658],\n",
       "       [-0.34770882],\n",
       "       [ 0.36178368],\n",
       "       [ 0.00393812],\n",
       "       [-0.04269557],\n",
       "       [-0.66145277],\n",
       "       [-0.63752776]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using autodiff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as above except for the `gradients = ...` line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gradients = tf.gradients(mse, [theta])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 9.16154\n",
      "Epoch 100 MSE = 0.714501\n",
      "Epoch 200 MSE = 0.566705\n",
      "Epoch 300 MSE = 0.555572\n",
      "Epoch 400 MSE = 0.548812\n",
      "Epoch 500 MSE = 0.543636\n",
      "Epoch 600 MSE = 0.539629\n",
      "Epoch 700 MSE = 0.536509\n",
      "Epoch 800 MSE = 0.534068\n",
      "Epoch 900 MSE = 0.532147\n",
      "Best theta:\n",
      "[[ 2.06855249]\n",
      " [ 0.88740271]\n",
      " [ 0.14401658]\n",
      " [-0.34770882]\n",
      " [ 0.36178368]\n",
      " [ 0.00393811]\n",
      " [-0.04269556]\n",
      " [-0.66145277]\n",
      " [-0.6375277 ]]\n"
     ]
    }
   ],
   "source": [
    "training_op = tf.assign(theta, theta - learning_rate * gradients)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval())\n",
    "        sess.run(training_op)\n",
    "    \n",
    "    best_theta = theta.eval()\n",
    "\n",
    "print(\"Best theta:\")\n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How could you find the partial derivatives of the following function with regards to `a` and `b`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_func(a, b):\n",
    "    z = 0\n",
    "    for i in range(100):\n",
    "        z = a * np.cos(z + i) + z * np.sin(b - i)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.21253923284754914"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_func(0.2, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "a = tf.Variable(0.2, name=\"a\")\n",
    "b = tf.Variable(0.3, name=\"b\")\n",
    "z = tf.constant(0.0, name=\"z0\")\n",
    "for i in range(100):\n",
    "    z = a * tf.cos(z + i) + z * tf.sin(b - i)\n",
    "\n",
    "grads = tf.gradients(z, [a, b])\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the function at $a=0.2$ and $b=0.3$, and the partial derivatives at that point with regards to $a$ and with regards to $b$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.212537\n",
      "[-1.1388494, 0.19671395]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    print(z.eval())\n",
    "    print(sess.run(grads))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a `GradientDescentOptimizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 9.16154\n",
      "Epoch 100 MSE = 0.714501\n",
      "Epoch 200 MSE = 0.566705\n",
      "Epoch 300 MSE = 0.555572\n",
      "Epoch 400 MSE = 0.548812\n",
      "Epoch 500 MSE = 0.543636\n",
      "Epoch 600 MSE = 0.539629\n",
      "Epoch 700 MSE = 0.536509\n",
      "Epoch 800 MSE = 0.534068\n",
      "Epoch 900 MSE = 0.532147\n",
      "Best theta:\n",
      "[[ 2.06855249]\n",
      " [ 0.88740271]\n",
      " [ 0.14401658]\n",
      " [-0.34770882]\n",
      " [ 0.36178368]\n",
      " [ 0.00393811]\n",
      " [-0.04269556]\n",
      " [-0.66145277]\n",
      " [-0.6375277 ]]\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval())\n",
    "        sess.run(training_op)\n",
    "    \n",
    "    best_theta = theta.eval()\n",
    "\n",
    "print(\"Best theta:\")\n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a momentum optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate,\n",
    "                                       momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best theta:\n",
      "[[ 2.06855798]\n",
      " [ 0.82962859]\n",
      " [ 0.11875337]\n",
      " [-0.26554456]\n",
      " [ 0.30571091]\n",
      " [-0.00450251]\n",
      " [-0.03932662]\n",
      " [-0.89986444]\n",
      " [-0.87052065]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        sess.run(training_op)\n",
    "    \n",
    "    best_theta = theta.eval()\n",
    "\n",
    "print(\"Best theta:\")\n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feeding data to the training algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Placeholder nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6.  7.  8.]]\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "A = tf.placeholder(tf.float32, shape=(None, 3))\n",
    "B = A + 5\n",
    "with tf.Session() as sess:\n",
    "    B_val_1 = B.eval(feed_dict={A: [[1, 2, 3]]})\n",
    "    B_val_2 = B.eval(feed_dict={A: [[4, 5, 6], [7, 8, 9]]})\n",
    "\n",
    "print(B_val_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  9.  10.  11.]\n",
      " [ 12.  13.  14.]]\n"
     ]
    }
   ],
   "source": [
    "print(B_val_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 1000\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n + 1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m / batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fetch_batch(epoch, batch_index, batch_size):\n",
    "    np.random.seed(epoch * n_batches + batch_index)  # not shown in the book\n",
    "    indices = np.random.randint(m, size=batch_size)  # not shown\n",
    "    X_batch = scaled_housing_data_plus_bias[indices] # not shown\n",
    "    y_batch = housing.target.reshape(-1, 1)[indices] # not shown\n",
    "    return X_batch, y_batch\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "\n",
    "    best_theta = theta.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.0703337 ],\n",
       "       [ 0.8637145 ],\n",
       "       [ 0.12255151],\n",
       "       [-0.31211874],\n",
       "       [ 0.38510373],\n",
       "       [ 0.00434168],\n",
       "       [-0.01232954],\n",
       "       [-0.83376896],\n",
       "       [-0.8030471 ]], dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and restoring a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 9.161543\n",
      "Epoch 100 MSE = 0.7145006\n",
      "Epoch 200 MSE = 0.56670463\n",
      "Epoch 300 MSE = 0.5555716\n",
      "Epoch 400 MSE = 0.5488117\n",
      "Epoch 500 MSE = 0.5436362\n",
      "Epoch 600 MSE = 0.53962916\n",
      "Epoch 700 MSE = 0.53650916\n",
      "Epoch 800 MSE = 0.5340678\n",
      "Epoch 900 MSE = 0.53214717\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_epochs = 1000                                                                       # not shown in the book\n",
    "learning_rate = 0.01                                                                  # not shown\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")            # not shown\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")            # not shown\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")                                      # not shown\n",
    "error = y_pred - y                                                                    # not shown\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")                                    # not shown\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)            # not shown\n",
    "training_op = optimizer.minimize(mse)                                                 # not shown\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval())                                # not shown\n",
    "            save_path = saver.save(sess, \"/tmp/my_model.ckpt\")\n",
    "        sess.run(training_op)\n",
    "    \n",
    "    best_theta = theta.eval()\n",
    "    save_path = saver.save(sess, \"/tmp/my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.0685525 ],\n",
       "       [ 0.8874027 ],\n",
       "       [ 0.14401658],\n",
       "       [-0.34770882],\n",
       "       [ 0.36178368],\n",
       "       [ 0.00393811],\n",
       "       [-0.04269556],\n",
       "       [-0.6614528 ],\n",
       "       [-0.6375277 ]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/my_model_final.ckpt\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"/tmp/my_model_final.ckpt\")\n",
    "    best_theta_restored = theta.eval() # not shown in the book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(best_theta, best_theta_restored)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to have a saver that loads and restores `theta` with a different name, such as `\"weights\"`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver({\"weights\": theta})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default the saver also saves the graph structure itself in a second file with the extension `.meta`. You can use the function `tf.train.import_meta_graph()` to restore the graph structure. This function loads the graph into the default graph and returns a `Saver` that can then be used to restore the graph state (i.e., the variable values):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/my_model_final.ckpt\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "# notice that we start with an empty graph.\n",
    "\n",
    "saver = tf.train.import_meta_graph(\"/tmp/my_model_final.ckpt.meta\")  # this loads the graph structure\n",
    "theta = tf.get_default_graph().get_tensor_by_name(\"theta:0\") # not shown in the book\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"/tmp/my_model_final.ckpt\")  # this restores the graph's state\n",
    "    best_theta_restored = theta.eval() # not shown in the book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(best_theta, best_theta_restored)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that you can import a pretrained model without having to have the corresponding Python code to build the graph. This is very handy when you keep tweaking and saving your model: you can load a previously saved model without having to search for the version of the code that built it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the graph\n",
    "## inside Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output, Image, display, HTML\n",
    "\n",
    "def strip_consts(graph_def, max_const_size=32):\n",
    "    \"\"\"Strip large constant values from graph_def.\"\"\"\n",
    "    strip_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = strip_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        if n.op == 'Const':\n",
    "            tensor = n.attr['value'].tensor\n",
    "            size = len(tensor.tensor_content)\n",
    "            if size > max_const_size:\n",
    "                tensor.tensor_content = b\"<stripped %d bytes>\"%size\n",
    "    return strip_def\n",
    "\n",
    "def show_graph(graph_def, max_const_size=32):\n",
    "    \"\"\"Visualize TensorFlow graph.\"\"\"\n",
    "    if hasattr(graph_def, 'as_graph_def'):\n",
    "        graph_def = graph_def.as_graph_def()\n",
    "    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
    "    code = \"\"\"\n",
    "        <script>\n",
    "          function load() {{\n",
    "            document.getElementById(\"{id}\").pbtxt = {data};\n",
    "          }}\n",
    "        </script>\n",
    "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "        <div style=\"height:600px\">\n",
    "          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "        </div>\n",
    "    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
    "\n",
    "    iframe = \"\"\"\n",
    "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n",
    "    \"\"\".format(code.replace('\"', '&quot;'))\n",
    "    display(HTML(iframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"\n",
       "        <script>\n",
       "          function load() {\n",
       "            document.getElementById(&quot;graph0.95071430641&quot;).pbtxt = 'node {\\n  name: &quot;X&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 20640\\n          }\\n          dim {\\n            size: 9\\n          }\\n        }\\n        tensor_content: &quot;<stripped 743040 bytes>&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 20640\\n          }\\n          dim {\\n            size: 1\\n          }\\n        }\\n        tensor_content: &quot;<stripped 82560 bytes>&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\t\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 42\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 42\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;random_uniform/max&quot;\\n  input: &quot;random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;random_uniform/RandomUniform&quot;\\n  input: &quot;random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;random_uniform/mul&quot;\\n  input: &quot;random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;theta&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 9\\n        }\\n        dim {\\n          size: 1\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;theta/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;theta&quot;\\n  input: &quot;random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@theta&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;theta/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;theta&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@theta&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;predictions&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;X&quot;\\n  input: &quot;theta/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;predictions&quot;\\n  input: &quot;y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Square&quot;\\n  op: &quot;Square&quot;\\n  input: &quot;sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\000\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;mse&quot;\\n  op: &quot;Mean&quot;\\n  input: &quot;Square&quot;\\n  input: &quot;Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Fill&quot;\\n  op: &quot;Fill&quot;\\n  input: &quot;gradients/Shape&quot;\\n  input: &quot;gradients/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mse_grad/Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\001\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mse_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Fill&quot;\\n  input: &quot;gradients/mse_grad/Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mse_grad/Tile/multiples&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\240P\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mse_grad/Tile&quot;\\n  op: &quot;Tile&quot;\\n  input: &quot;gradients/mse_grad/Reshape&quot;\\n  input: &quot;gradients/mse_grad/Tile/multiples&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tmultiples&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mse_grad/Shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\240P\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mse_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mse_grad/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/mse_grad/Shape&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mse_grad/Prod&quot;\\n  op: &quot;Prod&quot;\\n  input: &quot;gradients/mse_grad/Shape&quot;\\n  input: &quot;gradients/mse_grad/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/mse_grad/Shape&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mse_grad/Const_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/mse_grad/Shape&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mse_grad/Prod_1&quot;\\n  op: &quot;Prod&quot;\\n  input: &quot;gradients/mse_grad/Shape_1&quot;\\n  input: &quot;gradients/mse_grad/Const_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/mse_grad/Shape&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mse_grad/Maximum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/mse_grad/Shape&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mse_grad/Maximum&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;gradients/mse_grad/Prod_1&quot;\\n  input: &quot;gradients/mse_grad/Maximum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/mse_grad/Shape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mse_grad/floordiv&quot;\\n  op: &quot;FloorDiv&quot;\\n  input: &quot;gradients/mse_grad/Prod&quot;\\n  input: &quot;gradients/mse_grad/Maximum&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/mse_grad/Shape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mse_grad/Cast&quot;\\n  op: &quot;Cast&quot;\\n  input: &quot;gradients/mse_grad/floordiv&quot;\\n  attr {\\n    key: &quot;DstT&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;SrcT&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mse_grad/truediv&quot;\\n  op: &quot;RealDiv&quot;\\n  input: &quot;gradients/mse_grad/Tile&quot;\\n  input: &quot;gradients/mse_grad/Cast&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Square_grad/mul/x&quot;\\n  op: &quot;Const&quot;\\n  input: &quot;^gradients/mse_grad/truediv&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 2.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Square_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Square_grad/mul/x&quot;\\n  input: &quot;sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Square_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/mse_grad/truediv&quot;\\n  input: &quot;gradients/Square_grad/mul&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub_grad/Shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\240P\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\240P\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/sub_grad/Shape&quot;\\n  input: &quot;gradients/sub_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Square_grad/mul_1&quot;\\n  input: &quot;gradients/sub_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/sub_grad/Sum&quot;\\n  input: &quot;gradients/sub_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Square_grad/mul_1&quot;\\n  input: &quot;gradients/sub_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub_grad/Neg&quot;\\n  op: &quot;Neg&quot;\\n  input: &quot;gradients/sub_grad/Sum_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/sub_grad/Neg&quot;\\n  input: &quot;gradients/sub_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/sub_grad/Reshape&quot;\\n  input: &quot;^gradients/sub_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/sub_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/sub_grad/Reshape&quot;\\n  input: &quot;^gradients/sub_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/sub_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/sub_grad/Reshape_1&quot;\\n  input: &quot;^gradients/sub_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/sub_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/predictions_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/sub_grad/tuple/control_dependency&quot;\\n  input: &quot;theta/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/predictions_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;X&quot;\\n  input: &quot;gradients/sub_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/predictions_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/predictions_grad/MatMul&quot;\\n  input: &quot;^gradients/predictions_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/predictions_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/predictions_grad/MatMul&quot;\\n  input: &quot;^gradients/predictions_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/predictions_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/predictions_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/predictions_grad/MatMul_1&quot;\\n  input: &quot;^gradients/predictions_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/predictions_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/learning_rate&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.00999999977648\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_theta/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;theta&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;gradients/predictions_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@theta&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^GradientDescent/update_theta/ApplyGradientDescent&quot;\\n}\\nnode {\\n  name: &quot;init&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^theta/Assign&quot;\\n}\\nnode {\\n  name: &quot;save/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n        }\\n        string_val: &quot;model&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/SaveV2/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;theta&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/SaveV2/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/SaveV2&quot;\\n  op: &quot;SaveV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/SaveV2/tensor_names&quot;\\n  input: &quot;save/SaveV2/shape_and_slices&quot;\\n  input: &quot;theta&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;^save/SaveV2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@save/Const&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;theta&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2/tensor_names&quot;\\n  input: &quot;save/RestoreV2/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;theta&quot;\\n  input: &quot;save/RestoreV2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@theta&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/restore_all&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^save/Assign&quot;\\n}\\n';\n",
       "          }\n",
       "        </script>\n",
       "        <link rel=&quot;import&quot; href=&quot;https://tensorboard.appspot.com/tf-graph-basic.build.html&quot; onload=load()>\n",
       "        <div style=&quot;height:600px&quot;>\n",
       "          <tf-graph-basic id=&quot;graph0.95071430641&quot;></tf-graph-basic>\n",
       "        </div>\n",
       "    \"></iframe>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_graph(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n + 1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mse_summary = tf.summary.scalar('MSE', mse)\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m / batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:                                                        # not shown in the book\n",
    "    sess.run(init)                                                                # not shown\n",
    "\n",
    "    for epoch in range(n_epochs):                                                 # not shown\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "            if batch_index % 10 == 0:\n",
    "                summary_str = mse_summary.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                step = epoch * n_batches + batch_index\n",
    "                file_writer.add_summary(summary_str, step)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "\n",
    "    best_theta = theta.eval()                                                     # not shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.0703337 ],\n",
       "       [ 0.8637145 ],\n",
       "       [ 0.12255151],\n",
       "       [-0.31211874],\n",
       "       [ 0.38510373],\n",
       "       [ 0.00434168],\n",
       "       [-0.01232954],\n",
       "       [-0.83376896],\n",
       "       [-0.8030471 ]], dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name scopes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n + 1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\") as scope:\n",
    "    error = y_pred - y\n",
    "    mse = tf.reduce_mean(tf.square(error), name=\"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "mse_summary = tf.summary.scalar('MSE', mse)\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best theta:\n",
      "[[ 2.07033372]\n",
      " [ 0.86371452]\n",
      " [ 0.12255151]\n",
      " [-0.31211874]\n",
      " [ 0.38510373]\n",
      " [ 0.00434168]\n",
      " [-0.01232954]\n",
      " [-0.83376896]\n",
      " [-0.80304712]]\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "            if batch_index % 10 == 0:\n",
    "                summary_str = mse_summary.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                step = epoch * n_batches + batch_index\n",
    "                file_writer.add_summary(summary_str, step)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "\n",
    "    best_theta = theta.eval()\n",
    "\n",
    "file_writer.flush()\n",
    "file_writer.close()\n",
    "print(\"Best theta:\")\n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss/sub\n"
     ]
    }
   ],
   "source": [
    "print(error.op.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss/mse\n"
     ]
    }
   ],
   "source": [
    "print(mse.op.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "a_1\n",
      "param/a\n",
      "param_1/a\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "a1 = tf.Variable(0, name=\"a\")      # name == \"a\"\n",
    "a2 = tf.Variable(0, name=\"a\")      # name == \"a_1\"\n",
    "\n",
    "with tf.name_scope(\"param\"):       # name == \"param\"\n",
    "    a3 = tf.Variable(0, name=\"a\")  # name == \"param/a\"\n",
    "\n",
    "with tf.name_scope(\"param\"):       # name == \"param_1\"\n",
    "    a4 = tf.Variable(0, name=\"a\")  # name == \"param_1/a\"\n",
    "\n",
    "for node in (a1, a2, a3, a4):\n",
    "    print(node.op.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modularity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An ugly flat code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_features = 3\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "\n",
    "w1 = tf.Variable(tf.random_normal((n_features, 1)), name=\"weights1\")\n",
    "w2 = tf.Variable(tf.random_normal((n_features, 1)), name=\"weights2\")\n",
    "b1 = tf.Variable(0.0, name=\"bias1\")\n",
    "b2 = tf.Variable(0.0, name=\"bias2\")\n",
    "\n",
    "z1 = tf.add(tf.matmul(X, w1), b1, name=\"z1\")\n",
    "z2 = tf.add(tf.matmul(X, w2), b2, name=\"z2\")\n",
    "\n",
    "relu1 = tf.maximum(z1, 0., name=\"relu1\")\n",
    "relu2 = tf.maximum(z1, 0., name=\"relu2\")  # Oops, cut&paste error! Did you spot it?\n",
    "\n",
    "output = tf.add(relu1, relu2, name=\"output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better, using a function to build the ReLUs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "def relu(X):\n",
    "    w_shape = (int(X.get_shape()[1]), 1)\n",
    "    w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")\n",
    "    b = tf.Variable(0.0, name=\"bias\")\n",
    "    z = tf.add(tf.matmul(X, w), b, name=\"z\")\n",
    "    return tf.maximum(z, 0., name=\"relu\")\n",
    "\n",
    "n_features = 3\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "relus = [relu(X) for i in range(5)]\n",
    "output = tf.add_n(relus, name=\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_writer = tf.summary.FileWriter(\"logs/relu1\", tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even better using name scopes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "def relu(X):\n",
    "    with tf.name_scope(\"relu\"):\n",
    "        w_shape = (int(X.get_shape()[1]), 1)                          # not shown in the book\n",
    "        w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")    # not shown\n",
    "        b = tf.Variable(0.0, name=\"bias\")                             # not shown\n",
    "        z = tf.add(tf.matmul(X, w), b, name=\"z\")                      # not shown\n",
    "        return tf.maximum(z, 0., name=\"max\")                          # not shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_features = 3\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "relus = [relu(X) for i in range(5)]\n",
    "output = tf.add_n(relus, name=\"output\")\n",
    "\n",
    "file_writer = tf.summary.FileWriter(\"logs/relu2\", tf.get_default_graph())\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sharing Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sharing a `threshold` variable the classic way, by defining it outside of the `relu()` function then passing it as a parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "def relu(X, threshold):\n",
    "    with tf.name_scope(\"relu\"):\n",
    "        w_shape = (int(X.get_shape()[1]), 1)                        # not shown in the book\n",
    "        w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")  # not shown\n",
    "        b = tf.Variable(0.0, name=\"bias\")                           # not shown\n",
    "        z = tf.add(tf.matmul(X, w), b, name=\"z\")                    # not shown\n",
    "        return tf.maximum(z, threshold, name=\"max\")\n",
    "\n",
    "threshold = tf.Variable(0.0, name=\"threshold\")\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "relus = [relu(X, threshold) for i in range(5)]\n",
    "output = tf.add_n(relus, name=\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "def relu(X):\n",
    "    with tf.name_scope(\"relu\"):\n",
    "        if not hasattr(relu, \"threshold\"):\n",
    "            relu.threshold = tf.Variable(0.0, name=\"threshold\")\n",
    "        w_shape = int(X.get_shape()[1]), 1                          # not shown in the book\n",
    "        w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")  # not shown\n",
    "        b = tf.Variable(0.0, name=\"bias\")                           # not shown\n",
    "        z = tf.add(tf.matmul(X, w), b, name=\"z\")                    # not shown\n",
    "        return tf.maximum(z, relu.threshold, name=\"max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "relus = [relu(X) for i in range(5)]\n",
    "output = tf.add_n(relus, name=\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "with tf.variable_scope(\"relu\"):\n",
    "    threshold = tf.get_variable(\"threshold\", shape=(),\n",
    "                                initializer=tf.constant_initializer(0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"relu\", reuse=True):\n",
    "    threshold = tf.get_variable(\"threshold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"relu\") as scope:\n",
    "    scope.reuse_variables()\n",
    "    threshold = tf.get_variable(\"threshold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "def relu(X):\n",
    "    with tf.variable_scope(\"relu\", reuse=True):\n",
    "        threshold = tf.get_variable(\"threshold\")\n",
    "        w_shape = int(X.get_shape()[1]), 1                          # not shown\n",
    "        w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")  # not shown\n",
    "        b = tf.Variable(0.0, name=\"bias\")                           # not shown\n",
    "        z = tf.add(tf.matmul(X, w), b, name=\"z\")                    # not shown\n",
    "        return tf.maximum(z, threshold, name=\"max\")\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "with tf.variable_scope(\"relu\"):\n",
    "    threshold = tf.get_variable(\"threshold\", shape=(),\n",
    "                                initializer=tf.constant_initializer(0.0))\n",
    "relus = [relu(X) for relu_index in range(5)]\n",
    "output = tf.add_n(relus, name=\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_writer = tf.summary.FileWriter(\"logs/relu6\", tf.get_default_graph())\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "def relu(X):\n",
    "    with tf.variable_scope(\"relu\"):\n",
    "        threshold = tf.get_variable(\"threshold\", shape=(), initializer=tf.constant_initializer(0.0))\n",
    "        w_shape = (int(X.get_shape()[1]), 1)\n",
    "        w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")\n",
    "        b = tf.Variable(0.0, name=\"bias\")\n",
    "        z = tf.add(tf.matmul(X, w), b, name=\"z\")\n",
    "        return tf.maximum(z, threshold, name=\"max\")\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "with tf.variable_scope(\"\", default_name=\"\") as scope:\n",
    "    first_relu = relu(X)     # create the shared variable\n",
    "    scope.reuse_variables()  # then reuse it\n",
    "    relus = [first_relu] + [relu(X) for i in range(4)]\n",
    "output = tf.add_n(relus, name=\"output\")\n",
    "\n",
    "file_writer = tf.summary.FileWriter(\"logs/relu8\", tf.get_default_graph())\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "def relu(X):\n",
    "    threshold = tf.get_variable(\"threshold\", shape=(),\n",
    "                                initializer=tf.constant_initializer(0.0))\n",
    "    w_shape = (int(X.get_shape()[1]), 1)                        # not shown in the book\n",
    "    w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")  # not shown\n",
    "    b = tf.Variable(0.0, name=\"bias\")                           # not shown\n",
    "    z = tf.add(tf.matmul(X, w), b, name=\"z\")                    # not shown\n",
    "    return tf.maximum(z, threshold, name=\"max\")\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "relus = []\n",
    "for relu_index in range(5):\n",
    "    with tf.variable_scope(\"relu\", reuse=(relu_index >= 1)) as scope:\n",
    "        relus.append(relu(X))\n",
    "output = tf.add_n(relus, name=\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_writer = tf.summary.FileWriter(\"logs/relu9\", tf.get_default_graph())\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra material"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x0: my_scope/x\n",
      "x1: my_scope/x_1\n",
      "x2: my_scope/x_2\n",
      "x3: my_scope/x\n",
      "x4: my_scope_1/x\n",
      "x5: my_scope/x\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "with tf.variable_scope(\"my_scope\"):\n",
    "    x0 = tf.get_variable(\"x\", shape=(), initializer=tf.constant_initializer(0.))\n",
    "    x1 = tf.Variable(0., name=\"x\")\n",
    "    x2 = tf.Variable(0., name=\"x\")\n",
    "\n",
    "with tf.variable_scope(\"my_scope\", reuse=True):\n",
    "    x3 = tf.get_variable(\"x\")\n",
    "    x4 = tf.Variable(0., name=\"x\")\n",
    "\n",
    "with tf.variable_scope(\"\", default_name=\"\", reuse=True):\n",
    "    x5 = tf.get_variable(\"my_scope/x\")\n",
    "\n",
    "print(\"x0:\", x0.op.name)\n",
    "print(\"x1:\", x1.op.name)\n",
    "print(\"x2:\", x2.op.name)\n",
    "print(\"x3:\", x3.op.name)\n",
    "print(\"x4:\", x4.op.name)\n",
    "print(\"x5:\", x5.op.name)\n",
    "print(x0 is x3 and x3 is x5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first `variable_scope()` block first creates the shared variable `x0`, named `my_scope/x`. For all operations other than shared variables (including non-shared variables), the variable scope acts like a regular name scope, which is why the two variables `x1` and `x2` have a name with a prefix `my_scope/`. Note however that TensorFlow makes their names unique by adding an index: `my_scope/x_1` and `my_scope/x_2`.\n",
    "\n",
    "The second `variable_scope()` block reuses the shared variables in scope `my_scope`, which is why `x0 is x3`. Once again, for all operations other than shared variables it acts as a named scope, and since it's a separate block from the first one, the name of the scope is made unique by TensorFlow (`my_scope_1`) and thus the variable `x4` is named `my_scope_1/x`.\n",
    "\n",
    "The third block shows another way to get a handle on the shared variable `my_scope/x` by creating a `variable_scope()` at the root scope (whose name is an empty string), then calling `get_variable()` with the full name of the shared variable (i.e. `\"my_scope/x\"`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Do' 'you' 'want' 'some' 'caf\\xc3\\xa9?']\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "text = np.array(\"Do you want some cafÃ©?\".split())\n",
    "text_tensor = tf.constant(text)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(text_tensor.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a Home-Made Computation Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(x,y) = ((x) * (x)) * (y) + y + 2\n",
      "f(3,4) = 42\n"
     ]
    }
   ],
   "source": [
    "class Const(object):\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "    def evaluate(self):\n",
    "        return self.value\n",
    "    def __str__(self):\n",
    "        return str(self.value)\n",
    "\n",
    "class Var(object):\n",
    "    def __init__(self, init_value, name):\n",
    "        self.value = init_value\n",
    "        self.name = name\n",
    "    def evaluate(self):\n",
    "        return self.value\n",
    "    def __str__(self):\n",
    "        return self.name\n",
    "\n",
    "class BinaryOperator(object):\n",
    "    def __init__(self, a, b):\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "\n",
    "class Add(BinaryOperator):\n",
    "    def evaluate(self):\n",
    "        return self.a.evaluate() + self.b.evaluate()\n",
    "    def __str__(self):\n",
    "        return \"{} + {}\".format(self.a, self.b)\n",
    "\n",
    "class Mul(BinaryOperator):\n",
    "    def evaluate(self):\n",
    "        return self.a.evaluate() * self.b.evaluate()\n",
    "    def __str__(self):\n",
    "        return \"({}) * ({})\".format(self.a, self.b)\n",
    "\n",
    "x = Var(3, name=\"x\")\n",
    "y = Var(4, name=\"y\")\n",
    "f = Add(Mul(Mul(x, x), y), Add(y, Const(2))) # f(x,y) = xÂ²y + y + 2\n",
    "print(\"f(x,y) =\", f)\n",
    "print(\"f(3,4) =\", f.evaluate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing gradients\n",
    "### Mathematical differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df/dx(3,4) = 24\n",
      "df/dy(3,4) = 10\n"
     ]
    }
   ],
   "source": [
    "df_dx = Mul(Const(2), Mul(x, y))  # df/dx = 2xy\n",
    "df_dy = Add(Mul(x, x), Const(1))  # df/dy = xÂ² + 1\n",
    "print(\"df/dx(3,4) =\", df_dx.evaluate())\n",
    "print(\"df/dy(3,4) =\", df_dy.evaluate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df/dx(3,4) = 24.0004\n",
      "df/dy(3,4) = 10.0\n"
     ]
    }
   ],
   "source": [
    "def gradients(func, vars_list, eps=0.0001):\n",
    "    partial_derivatives = []\n",
    "    base_func_eval = func.evaluate()\n",
    "    for var in vars_list:\n",
    "        original_value = var.value\n",
    "        var.value = var.value + eps\n",
    "        tweaked_func_eval = func.evaluate()\n",
    "        var.value = original_value\n",
    "        derivative = (tweaked_func_eval - base_func_eval) / eps\n",
    "        partial_derivatives.append(derivative)\n",
    "    return partial_derivatives\n",
    "\n",
    "df_dx, df_dy = gradients(f, [x, y])\n",
    "print(\"df/dx(3,4) =\", df_dx)\n",
    "print(\"df/dy(3,4) =\", df_dy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Symbolic differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df/dx(3,4) = 24.0\n",
      "df/dy(3,4) = 10.0\n"
     ]
    }
   ],
   "source": [
    "Const.derive = lambda self, var: Const(0)\n",
    "Var.derive = lambda self, var: Const(1) if self is var else Const(0)\n",
    "Add.derive = lambda self, var: Add(self.a.derive(var), self.b.derive(var))\n",
    "Mul.derive = lambda self, var: Add(Mul(self.a, self.b.derive(var)), Mul(self.a.derive(var), self.b))\n",
    "\n",
    "x = Var(3.0, name=\"x\")\n",
    "y = Var(4.0, name=\"y\")\n",
    "f = Add(Mul(Mul(x, x), y), Add(y, Const(2))) # f(x,y) = xÂ²y + y + 2\n",
    "\n",
    "df_dx = f.derive(x)  # 2xy\n",
    "df_dy = f.derive(y)  # xÂ² + 1\n",
    "print(\"df/dx(3,4) =\", df_dx.evaluate())\n",
    "print(\"df/dy(3,4) =\", df_dy.evaluate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic differentiation (autodiff) â€“ forward mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DualNumber(object):\n",
    "    def __init__(self, value=0.0, eps=0.0):\n",
    "        self.value = value\n",
    "        self.eps = eps\n",
    "    def __add__(self, b):\n",
    "        return DualNumber(self.value + self.to_dual(b).value,\n",
    "                          self.eps + self.to_dual(b).eps)\n",
    "    def __radd__(self, a):\n",
    "        return self.to_dual(a).__add__(self)\n",
    "    def __mul__(self, b):\n",
    "        return DualNumber(self.value * self.to_dual(b).value,\n",
    "                          self.eps * self.to_dual(b).value + self.value * self.to_dual(b).eps)\n",
    "    def __rmul__(self, a):\n",
    "        return self.to_dual(a).__mul__(self)\n",
    "    def __str__(self):\n",
    "        if self.eps:\n",
    "            return \"{:.1f} + {:.1f}Îµ\".format(self.value, self.eps)\n",
    "        else:\n",
    "            return \"{:.1f}\".format(self.value)\n",
    "    def __repr__(self):\n",
    "        return str(self)\n",
    "    @classmethod\n",
    "    def to_dual(cls, n):\n",
    "        if hasattr(n, \"value\"):\n",
    "            return n\n",
    "        else:\n",
    "            return cls(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$3 + (3 + 4 \\epsilon) = 6 + 4\\epsilon$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'ascii' codec can't encode character u'\\u03b5' in position 9: ordinal not in range(128)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m--------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m     Traceback (most recent call last)",
      "\u001b[1;32m/home/weiwu/.virtualenvs/nlp/local/lib/python2.7/site-packages/IPython/core/formatters.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    697\u001b[0m                 \u001b[0mtype_pprinters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype_printers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m                 deferred_pprinters=self.deferred_printers)\n\u001b[1;32m--> 699\u001b[1;33m             \u001b[0mprinter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpretty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    700\u001b[0m             \u001b[0mprinter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/weiwu/.virtualenvs/nlp/local/lib/python2.7/site-packages/IPython/lib/pretty.pyc\u001b[0m in \u001b[0;36mpretty\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    381\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmeth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m                                 \u001b[1;32mreturn\u001b[0m \u001b[0mmeth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 383\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0m_default_pprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    384\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    385\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend_group\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/weiwu/.virtualenvs/nlp/local/lib/python2.7/site-packages/IPython/lib/pretty.pyc\u001b[0m in \u001b[0;36m_default_pprint\u001b[1;34m(obj, p, cycle)\u001b[0m\n\u001b[0;32m    501\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_safe_getattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mklass\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'__repr__'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_baseclass_reprs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    502\u001b[0m         \u001b[1;31m# A user-provided repr. Find newlines and replace them with p.break_()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 503\u001b[1;33m         \u001b[0m_repr_pprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    504\u001b[0m         \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    505\u001b[0m     \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin_group\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'<'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/weiwu/.virtualenvs/nlp/local/lib/python2.7/site-packages/IPython/lib/pretty.pyc\u001b[0m in \u001b[0;36m_repr_pprint\u001b[1;34m(obj, p, cycle)\u001b[0m\n\u001b[0;32m    692\u001b[0m     \u001b[1;34m\"\"\"A pprint that just redirects to the normal repr function.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    693\u001b[0m     \u001b[1;31m# Find newlines and replace them with p.break_()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 694\u001b[1;33m     \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrepr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    695\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moutput_line\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    696\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-106-75e762d70dc0>\u001b[0m in \u001b[0;36m__repr__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;34m\"{:.1f}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mto_dual\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m: 'ascii' codec can't encode character u'\\u03b5' in position 9: ordinal not in range(128)"
     ]
    }
   ],
   "source": [
    "3 + DualNumber(3, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$(3 + 4Îµ)\\times(5 + 7Îµ) = 3 \\times 5 + 3 \\times 7Îµ + 4Îµ \\times 5 + 4Îµ \\times 7Îµ = 15 + 21Îµ + 20Îµ + 28Îµ^2 = 15 + 41Îµ + 28 \\times 0 = 15 + 41Îµ$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'ascii' codec can't encode character u'\\u03b5' in position 11: ordinal not in range(128)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m--------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m     Traceback (most recent call last)",
      "\u001b[1;32m/home/weiwu/.virtualenvs/nlp/local/lib/python2.7/site-packages/IPython/core/formatters.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    697\u001b[0m                 \u001b[0mtype_pprinters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype_printers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m                 deferred_pprinters=self.deferred_printers)\n\u001b[1;32m--> 699\u001b[1;33m             \u001b[0mprinter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpretty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    700\u001b[0m             \u001b[0mprinter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/weiwu/.virtualenvs/nlp/local/lib/python2.7/site-packages/IPython/lib/pretty.pyc\u001b[0m in \u001b[0;36mpretty\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    381\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmeth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m                                 \u001b[1;32mreturn\u001b[0m \u001b[0mmeth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 383\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0m_default_pprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    384\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    385\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend_group\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/weiwu/.virtualenvs/nlp/local/lib/python2.7/site-packages/IPython/lib/pretty.pyc\u001b[0m in \u001b[0;36m_default_pprint\u001b[1;34m(obj, p, cycle)\u001b[0m\n\u001b[0;32m    501\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_safe_getattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mklass\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'__repr__'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_baseclass_reprs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    502\u001b[0m         \u001b[1;31m# A user-provided repr. Find newlines and replace them with p.break_()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 503\u001b[1;33m         \u001b[0m_repr_pprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    504\u001b[0m         \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    505\u001b[0m     \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin_group\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'<'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/weiwu/.virtualenvs/nlp/local/lib/python2.7/site-packages/IPython/lib/pretty.pyc\u001b[0m in \u001b[0;36m_repr_pprint\u001b[1;34m(obj, p, cycle)\u001b[0m\n\u001b[0;32m    692\u001b[0m     \u001b[1;34m\"\"\"A pprint that just redirects to the normal repr function.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    693\u001b[0m     \u001b[1;31m# Find newlines and replace them with p.break_()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 694\u001b[1;33m     \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrepr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    695\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moutput_line\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    696\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-106-75e762d70dc0>\u001b[0m in \u001b[0;36m__repr__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;34m\"{:.1f}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mto_dual\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m: 'ascii' codec can't encode character u'\\u03b5' in position 11: ordinal not in range(128)"
     ]
    }
   ],
   "source": [
    "DualNumber(3, 4) * DualNumber(5, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42.0"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.value = DualNumber(3.0)\n",
    "y.value = DualNumber(4.0)\n",
    "\n",
    "f.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x.value = DualNumber(3.0, 1.0)  # 3 + Îµ\n",
    "y.value = DualNumber(4.0)       # 4\n",
    "\n",
    "df_dx = f.evaluate().eps\n",
    "\n",
    "x.value = DualNumber(3.0)       # 3\n",
    "y.value = DualNumber(4.0, 1.0)  # 4 + Îµ\n",
    "\n",
    "df_dy = f.evaluate().eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.0"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autodiff â€“ Reverse mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(x,y) = ((x) * (x)) * (y) + y + 2\n",
      "f(3,4) = 42\n",
      "df_dx = 24.0\n",
      "df_dy = 10.0\n"
     ]
    }
   ],
   "source": [
    "class Const(object):\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "    def evaluate(self):\n",
    "        return self.value\n",
    "    def backpropagate(self, gradient):\n",
    "        pass\n",
    "    def __str__(self):\n",
    "        return str(self.value)\n",
    "\n",
    "class Var(object):\n",
    "    def __init__(self, init_value, name):\n",
    "        self.value = init_value\n",
    "        self.name = name\n",
    "        self.gradient = 0\n",
    "    def evaluate(self):\n",
    "        return self.value\n",
    "    def backpropagate(self, gradient):\n",
    "        self.gradient += gradient\n",
    "    def __str__(self):\n",
    "        return self.name\n",
    "\n",
    "class BinaryOperator(object):\n",
    "    def __init__(self, a, b):\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "\n",
    "class Add(BinaryOperator):\n",
    "    def evaluate(self):\n",
    "        self.value = self.a.evaluate() + self.b.evaluate()\n",
    "        return self.value\n",
    "    def backpropagate(self, gradient):\n",
    "        self.a.backpropagate(gradient)\n",
    "        self.b.backpropagate(gradient)\n",
    "    def __str__(self):\n",
    "        return \"{} + {}\".format(self.a, self.b)\n",
    "\n",
    "class Mul(BinaryOperator):\n",
    "    def evaluate(self):\n",
    "        self.value = self.a.evaluate() * self.b.evaluate()\n",
    "        return self.value\n",
    "    def backpropagate(self, gradient):\n",
    "        self.a.backpropagate(gradient * self.b.value)\n",
    "        self.b.backpropagate(gradient * self.a.value)\n",
    "    def __str__(self):\n",
    "        return \"({}) * ({})\".format(self.a, self.b)\n",
    "\n",
    "x = Var(3, name=\"x\")\n",
    "y = Var(4, name=\"y\")\n",
    "f = Add(Mul(Mul(x, x), y), Add(y, Const(2))) # f(x,y) = xÂ²y + y + 2\n",
    "\n",
    "result = f.evaluate()\n",
    "f.backpropagate(1.0)\n",
    "\n",
    "print(\"f(x,y) =\", f)\n",
    "print(\"f(3,4) =\", result)\n",
    "print(\"df_dx =\", x.gradient)\n",
    "print(\"df_dy =\", y.gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autodiff â€“ reverse mode (using TensorFlow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42.0, [24.0, 10.0])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "x = tf.Variable(3., name=\"x\")\n",
    "y = tf.Variable(4., name=\"y\")\n",
    "f = x*x*y + y + 2\n",
    "\n",
    "gradients = tf.gradients(f, [x, y])\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    f_val, gradients_val = sess.run([f, gradients])\n",
    "\n",
    "f_val, gradients_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. to 11."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "See appendix A."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Logistic Regression with Mini-Batch Gradient Descent using TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's create the moons dataset using Scikit-Learn's `make_moons()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "\n",
    "m = 1000\n",
    "X_moons, y_moons = make_moons(m, noise=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a peek at the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAD/CAYAAADi+OGRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJztvXucVdV5//95ZjiHmWGQwEDV1syQ\nfIOajshEx8aEijY0RvBnVfxK1AHGJJY4UxvTpib4I4kXStrYptH4U5BEEJipX21+KBrB2BAveEtF\nAREbMVGHWkYLgw7OBc5cnu8f+6yZdfZZa++199nnOuv9eu0XzD77ss4+e+9nPXdiZlgsFovF4kdZ\nvgdgsVgsluLACgyLxWKxGGEFhsVisViMsALDYrFYLEZYgWGxWCwWI6zAsFgsFosRVmBYLBaLxQgr\nMCwWi8VihBUYFovFYjFiXL4HECVTp07l6dOn53sYFovFUjS8/PLLh5h5msm2JSUwpk+fjh07duR7\nGBaLxVI0EFGH6bbWJGWxWCwWIyITGER0HRHtIKJjRHSfx3bNRPQyER0honeJ6DYiGid9/hQRHSWi\nnuTyRlRjtFgsFkt4otQwDgD4ewBrfbarAvBNAFMBfBbAXAB/59rmOmauTi6nRDhGi8VisYQkMh8G\nM28CACJqBHCSx3arpD//m4jaAfxZVOOwWCyly8DAAN59910cPXo030MpOioqKnDSSSchFouFPkYh\nOL3nANjrWvcPRPSPAN4AsJyZn9LtTERLASwFgNra2myN0WKxFADvvvsuJk6ciOnTp4OI8j2cooGZ\n0dXVhXfffRef+MQnQh8nr05vIvoqgEYA/yyt/g6ATwL4IwBrADxKRP9LdwxmXsPMjczcOG2aUWSY\nxWJOZydw7rnAe+/leyQWAEePHkVNTY0VFgEhItTU1GSsmeVNYBDRJQD+AcA8Zj4k1jPzb5j5I2Y+\nxszrATwHYH6+xmkZ46xYATz7rPOvpSCwwiIcUVy3vAgMIroAwE8BXMTMe3w2ZwD2DrHkns5OYN06\nYHjY+ddqGZYxTpRhteOIqAJAOYByIqqQw2Wl7b4AoB3AZcz8H67PPkZEXxL7ElETHB/H41GN02Ix\nZsUKR1gAwNCQ1TIsKC8vR0NDA0477TRcfvnl6OvrC3yMa665Bq+//joA4Ac/+EHKZ5///OcjGWe2\niFLD+C6AfgDLACxK/v+7RFSbzKcQHunvAZgEYIuUa7E1+VkMTmjuQQCHAPw1gEuYeV+E47RY/BHa\nRSLh/J1IWC2jCGnf047pt09H2S1lmH77dLTvac/oeJWVldi1axdee+01xONxrF69OvAxfvazn+GP\n//iPAaQLjOeffz6j8WWbyAQGM9/MzORabmbm/cl8iv3J7f6MmcdJeRbVzDwv+dlBZj6LmScy88eY\n+Wxm/veoxmgpQvLldJa1C4HVMoqK9j3tWProUnR0d4DB6OjuwNJHl2YsNATnnHMOfve73wEA/uVf\n/gWnnXYaTjvtNNx+++0AgN7eXlx44YWYNWsWTjvtNDzwwAMAgPPOOw87duzAsmXL0N/fj4aGBjQ1\nNQEAqqurAQBXXHEFHnvssZFzXX311fj5z3+OoaEh3HDDDTjrrLNw+umn45577onku5hiS4NYCpts\nOJ1lIaQTSC+8MKpdCBIJoMBngJZRlm9bjr6BVJNR30Aflm9bnvGxBwcHsXXrVsycORMvv/wy1q1b\nh9/85jd48cUX8dOf/hQ7d+7E448/jj/8wz/E7t278dprr+GCCy5IOcY//uM/jmgs7e2pQuzLX/4y\nHnzwQQBAIpHAtm3bcOGFF+Lee+/FpEmT8NJLL+Gll17CT3/6U7z99tsZfx9TrMCwFC7ZcjrLQkgl\nkDo7geOOc/5lTl127oxmDJass797f6D1JgiNoLGxEbW1tfja176GZ599FpdeeikmTJiA6upqLFiw\nANu3b8fMmTPx7//+7/jOd76D7du3Y9KkScbnmTdvHp588kkcO3YMW7duxZw5c1BZWYknnngCGzZs\nQENDAz772c+iq6sLb775ZujvExQrMMYyhZ5jkA2nsyyE1q51FrdAsqG0JUHtJHUir269CUIj2LVr\nF+68807E43HttieffDJeeeUVzJw5E9/97ndx6623Gp+noqIC5513Hn75y1/igQcewJe//GUATgLe\nnXfeOTKGt99+G+eff37o7xMUKzDGMoX4YhRCbPfudKfz2rWZCzdZCCUSwMCA838hkGwobcmwcu5K\nVMWqUtZVxaqwcu7KSM9zzjnn4OGHH0ZfXx96e3vx0EMP4ZxzzsGBAwdQVVWFRYsW4YYbbsArr7yS\ntm8sFsOAuAddfPnLX8a6deuwffv2EXPWl770JaxatWpkn3379qG3tzfS7+MJM5fMcuaZZ7LFkAMH\nmCsqHENLZSVzZ2fmx5szJ/PjtLQwl5Ux19czx+OpBqGyMubm5vDnkb+zaqmsdI4vzhuPM7e2Bj9H\nFNfBouT1118PtH3bq21c9+M6ppuJ635cx22vtmV0/gkTJijX/+hHP+L6+nqur6/nH//4x8zM/Pjj\nj/PMmTN51qxZ3NjYyC+99BIzM5977rkj///2t7/Np556Kl911VVpx08kEjx58mS++uqrR9YNDQ3x\njTfeyKeddhrX19fzeeedxx9++KHx+FXXD8AONnzH5v0lH+ViBUYAWloyezGqjldWltlx5Bc6kfql\nPmmS829zc/DjL1miP664DuXl6UIkyMs/iutg0RJUYFhSyVRgWJPUWCTqHIOozDiyuSgWA1pbgZYW\nQNiJYzHgyBHn/21t/udx+2gee8wRAzoSCcc0JRPEd2LNWZYSxwqMsUjUOQZ+zmmVc929TiXEhFNa\nrBsYGH3hDw0BDQ3eL2XZR9PZCQhbb2Wl8/eBA0BFxei6+vr0YwQJpbWZ4ZYSxwqMsUiUOQadnakv\ndZW2onKuu9ephJjslFbx/vvAjTfqxyXP9m+8Mf1l7n7Bn3uu2lhlEkprM8MtYwFT21UxLNaHIZEr\n56uw2bt9AcKGv3Pn6OfCH6ByuDc06H0LXkt5ufo7un00bt9ERUW6AzwT5798PtV1sESC9WFkhvVh\njDVMcydyFTL7zDNqzUBoK4sWec/sjx4Fli1zZvEHDgBz5qgT5mpq1OcfGnL2/9znnGX3buDss9Nn\n+27fRCKRrmVlYkaymeGWsYCpZCmGZUxoGCZROFGHzPqNRxdttXOnWitwz8SFlqD6bkJTOvlkM42j\nvp5HQnDDaCwNDWbf24bP5gWrYWSG1TDGEqZROLlyvvrZ7RctUu+nmtlff736uwlNKRYDygxu173J\nbr9urQdwnORCNBw4AJx4IkDk/Cu0GtPSH4WY9GjJOkSEb33rWyN///M//zNuvvnmyM9TqGXPrcAo\nJkwEQS6dr7poqzPOcExDyZr/RmzenP7dZAG5d69aCOgoK3PCcoVwmDMH2Lp19PNly0aFRGen3nmu\nImj4bKGXYCl1Irz+48ePx6ZNm3Do0CH/jTOgUMueW4FRLJgKghUrMsslCILObt/ZCTQ1OVqBKceO\npX83ObLJRLuQEbWi3nsvXRvo7ARc1UGxcaPjAzF5qZhqcOJFdeONVhvJJxFqg+PGjcPSpUvx4x//\nOO2zgwcP4rLLLsNZZ52Fs846C88999zI+i9+8Yuor6/HNddcg7q6uhGBc8kll+DMM89EfX091qxZ\nAwCFXfbc1HZlsgC4DsAOAMcA3Oez7d8AeA/AEQBrAYyXPpsO4EkAfQB+C+DPTc5f0j4M0ygcXbSR\nqW0+LAcOMJ99NvP48c75vDKqTZZYLD2yKehCxHz88aNjEv6cJUv0++gyyIXPYteu9OiqsjLm3bvT\n92lpccYgvke2/UljgMA+jIj9eRMmTODu7m6uq6vjDz/8kP/pn/6Jb7rpJmZmvvLKK3n79u3MzNzR\n0cGnnnoqMzP/1V/9Ff/gBz9gZuatW7cyAD548CAzM3d1dTEzc19fH9fX1/OhQ4dGzuM+LzPzpk2b\neMmSJczMfOzYMT7ppJO4r6+P77nnHl6xYgUzMx89epTPPPNMfuutt9LGX2g+jANwOuat9dqIiL4E\npzPfXAB1AD4J4BZpk/sB7ARQA2A5gJ8T0bSIx1pcmEbhbNmSmowW1Davwk+l7+wEzjwTePHF0byJ\nWMxJhPOo5unJwEC6phQUZidXQy4wuGxZunYho8sgF7PUpqZ009jwMLBwYeo6oREyj34P2dRmTVS5\nIQv+vOOOOw5LlizBT37yk5T1v/rVr3DdddehoaEBf/EXf4EjR46gp6cHzz77LK644goAwAUXXIDJ\nkyeP7POTn/wEs2bNwtlnn43/+q//8i1Vnvey56aSJcgCR2hoNQwA/wrgB9LfcwG8l/z/yXA0lInS\n59sBXOt33pLWMEwJUiPKNNJHjl5S7aObsftpGRUV+vGEzcvwW0y0FreWYVLjCki9Ji0tjpbk3kYU\nOLT1pkIRSMNQFZvMUMsQM/2uri6uq6vjm2++eUTDqKmp4f7+/rR9Zs2alTLbnzx5Mh88eJCffPJJ\nnj17Nvf29jKzU5TwySefTDmP+7zMzIsXL+bNmzfzlVdeyZs3b2Zm5gULFvDjjz/uO/5C0zBMqQew\nW/p7N4Djiagm+dlbzPyR63NF3QZLCkEd3jrbrrsjnTtj2s8fIIjFgPJy5//jxwPV1c6/gKP9uDuF\nyePZudN5xFtanEgmcRxBPD6qPalKeugw0Vo2bkzPVHfXuGIGlixJ3W/ZMudfcc1UWeqDg44WY+tN\nZZ8sttmdMmUKFi5ciHvvvXdk3fnnn48777xz5O9du3YBAGbPnj3SPe+JJ57ABx98AADo7u7G5MmT\nUVVVhd/+9rd48cUXR/Yt2LLnppIlyAJ/DeP3AC6Q/o4BYDi+i8UAXnRtv1J3PABL4fhNdtTW1vpK\n2JImSLax27a7a9fo7F7WKOQKr7JfwcQf4LW4x6WyNfuVIxf7q76311JT45+n0dyc7peRZ6m7dqVr\nK3I+icl4bCZ4YAJpGFnw58kz/ffee48rKytHNIyDBw/ywoULeebMmfzpT3+av/71rzMz8/vvv89f\n+MIXuL6+nq+55ho+4YQT+OjRo3z06FG+4IIL+NRTT+WLL744RcPIVtnzgixvbiAwdgNYKP1dkxQY\nNQAuBfC6a/s7Adzpd94xb5IK8oC4TVf19aP9JsQL0v2idL/smpvVJp5YTP+ZzjygMqX5vXinT3eE\nnEjW81pkE5DJ9lOmOOcX+7q/+ymn6AWNlznNfSzrCA9EMSbuHT16lAcGBpiZ+fnnn+dZs2blbSzF\nKjD+FcBK6e8vINWHcRSpPoxnYH0Y0eE1cy8vN49w8hIIkyf77y8Eg2o8qlpPqv1VvgCdQKivdz7X\n+Rfc4/c7v2qZOjV1LH5Cz2oZgShGgbFv3z5uaGjg008/nRsbG/k//uM/8jaWgvJhENE4IqoAUA6g\nnIgqiGicYtMNAL5GRH9MRB8D8F0A9wEAM+8DsAvATcn9LwVwOoD/P8qxFh2mkTWdnU4tJVFXSbWP\nyrYrGBpyXmUm6PwB9fVmeRMiyks1nmPHnDpTfvurfAFz5qRHZ8XjzrUAnIgzryq4gNN3Q/5+9fWj\nr/mWFv1+xx8/+n+3T0n3HcImZdloq6JgxowZ2LlzJ3bv3o2XXnoJZ511Vr6HFB5TyWKyALgZALuW\nmwHUAugBUCtt+7cA3oeTh7EO6XkYTwHoB/AGbB6GeSc3YUYRM2pVdFO2IpCAUXOU3+xczPaZMx+P\ne5ZuYpoLkyeye7c+rl/1++i0i/JyxzcSVTvbMaShvP766zw8PJzvYRQlw8PDhWmSytdSsgLDNPno\nwAG130GEchIxn3hievhnEIexyVJT439MIv33kB3p4vsG8YeYXM8w3+vkk9W+FlUJd2Z/QRhVO9sx\n5Ad56623+ODBg1ZoBGR4eJgPHjyoTOYLIjDI2b40aGxs5B07duR7GNHT2grce69jvojHgWuuAe66\nS73dPfekm3ficce8Ikwszc3Affc5///MZ4Bk+F9oYjHnnENDTrjsJz85WgRQZsIE4He/A2691Rnn\ntdemf4/OTuDjHx8daywGXHUV8MAD3iYqr+viprUVWLXK/PvJjB/vmMsElZVAbS3wxhvqcXhd38pK\nxzz2jW843++EE8zHYXpPlBgDAwN49913cdTPXGlJo6KiAieddBJirpI9RPQyMzcaHcRUshTDUpIa\nhmnykU678DKvuInCVCWip9zJfcJ8IpurVN9DFaZbXu7vpHabnHTX8rOfNXNml5WpzVbudePGpW9T\nUTH6veSSImefnRolReRoLaZmJa/yJGNIy7BEC6xJqoQwza1Qdb7zWk45RZ217Ta5mITHql7wRKNj\nlIVeebk+E/3AgfD1o2SfiHw8+fvpwmSjXsrKRk1/4nfxC+WVhYzXvSCOZbv7WSLCCoxSwjS3Iox2\ncPnlqbNblTaTSQFAMev18pO4Hce6Y02d6h2qq/KJuEuahAmT9RrvgQPeznMT578sZEybYunOme0C\nk5aSxAqMsUpQLUO8eCoqHHNJc3O0DnChoXi9NOWZsZfQGz/eW3jJx3FnaAunv/zd6uvDCVn5PH4B\nA7I2ZbK4tQxZQwpSI8xiCYAVGGOVsD4IIWR0M/gJE5yXlpdZJR5Xt1E1eWnKM+NMorZkjUb+XrpS\n6So/DrN/uZOGBudlPmFCuHF6/Q7usNyyslFNUPVdLZYMCSIwbAOlUkIU7GN2ksvKypyEMyLv/URU\nVXf3aEE/sSxZAvT2OkUHVQlxgkQC2Lcvff3QkDpxTU6Ek0uvq8q4myLKl69dm/q9dKXSr7oqfd2u\nXcCGDfpzNDQ4412xAujvd6KVghQ/9GJ4GHj6aef/ctHHf/u3rBXRs1gCYSpZimEZ8xqGIBN7vVza\nW3ZCl5frNYyGBm//Q0NDahFDE5NKWE3DpLCgWFR+D5WW5NaEdu5MNedl2ixKLLFYsIKK1mdhiQBY\nk9QYJ9NkPGGqcZtmpOqYKcgvUJXJRBX95GdSCWpeq6hgPuOMYKHFqigtL0EkkAVnWVm0UVfC3KUS\n+LJAsVgiwgqMUsWk4ZGpdlFWpq+4KkJudeW73ag0D/llrPIJRK1liJe2++UtXrIm0WbucV5+eXrO\nyM6d3uNwC8KaGrPxy/uaRpWp7gnTplgWSxIrMIoRkwfdpHZQVKU+Lr9cvd6tZXiFloqXse6l6WVS\nCVvBNuh55POpHOPuyCS/fArTulZe+3rtE4s5OR67djlJiH/wB6k5L2OwvpQlM6zAKEb8HnTT2kFR\nFRbUmXZk04wYt1e4Z9g2mSrBpzL/iF4e7mTDIHWWDhxwXrx+18RUWNXXpycMmgpx01pUbsElmjqN\nwfpSlsywAqPY0AkDrzh8VfkN3TG9Fp12YJIcZiIMgnQBlAki+Nxj9comV+HlsDcRWLqse5P8Evfi\nNV6vcF+V4LRahsUAKzCKDd0sXVV/SX4pyqYImZ07HdOFV/0l9wsmyItLNW7dfllok+k7Bq9Zu5so\nMsB1QlQn/HXmPll7cPsl3HWoTBarZVgMsAKjmNDN0mXzglfyG9FonwbxkjFpQRrkRajyr3glrkUZ\n7unn2zGZvXsJP5MMahNzoV9GttAOLr/cOwxXZKXL5zPVgMIIfcuYJ28CA8AUAA8B6AXQAeAqzXZb\n4TRUEksCwB7p83fgNE8Snz9hcv6iFBi6WbrX7F81I5WzguXPLr/c/Di6F4x42ckOb532owu9zeT6\nBHHiBtFoTExqJr4jL01QhPua1uSSs9LFxCFIqHA2hbelJMmnwLgfwAMAqgH8KYBuAPUG+z0F4PvS\n3+/AsMuevBSlwNC94FR2eaJgBe2A4C8b9wvGnbzX2ZlefVY2felCb90EDRHOhnnFxKQWxKmvKsOe\nSY6GmDiYHkMkLVqtwhKAvAgMABOSmsLJ0rqNAP7RZ7/pAIYATJfWjR2BocLLLm/aG0JefvWr0Zdz\n0Dh9VfKen9/ARMvw0hzEGOWCgdkwr/hpI2Gd+rlaTHwnFosP+RIYnwHQ51r3dwAe9dnv+wCecq17\nB06/74MAngAwy2QMJSMwou65PWnSaHtWYR/3i7JiVucmlJX5azh+Wobfy62lxRlv0OzwqPHTQHS5\nIu4+31H+lvH4aK8NW83WEgH5EhjnAHjPte4v3cJAsd/vAFztWjcbQCWAKgA3AngPwMc0+y8FsAPA\njtra2mxcz/yhc14HLZvtfuGLY+iirAS6ME6T2kmmyYWq8hw6gZTrF6GfBqLLFYmy/4bX9fXynVgt\nw2JIIWkY3/LSMJJ+jh4A1T7H/i2Ai/zGUHAaRiZlGvxKUJjORv22Eb0wVGMMUtZC91JVXROvl5uf\niaeQnLg6gSKCEMIIdd33E5MHueih8E+pzJRWy7AYEkRgRFnefB+AcUQ0Q1o3C8Bej32aAWxi5h6f\nYzMAnxrdBciKFcCzz4YrQ71okffnDQ2jJcx1mJQJTySAF19Uj/HjH1fvI8qlV1aml0MXi1yyXGbF\nCn2pblHSWx63+xy64+YDUU6+pWW07Hs8Dpx7rr5Me0ODt8jYudP5vueeC7z3nvP/M88E9iYfI2cC\n5Rx7YMD5/9DQ6P8FiQTw/PPZ+d6WsYupZDFZAPwfOJFSE+CYlbRRUnBMTt0AvuBaX5vcNw6gAsAN\ncHwZNX7nLygNIxMnpFd9pqlTR7eLKt9CzFhNxpiprdzLzBM2KzyfhC194oUcEODXzCmqc1rGLMhz\nHsbDcPIw9iOZhwHHv9Hj2vZKOLka5FpfD+DV5DG6AGwD0Ghy/rwLjKickEF6YEflVDUJx9y5M7ud\n37KdFZ4NohZysgCqqDD/fWVnuMUSgLwJjHwveRcYfglcpg+zV5SU/DKKUsMwefn7lTEfi0Qt5GQB\nFKYx01j+LSyhCCIwbIvWqJBbara1pbcETSTMfRnCNq5q/Snbps88M/WzU05Rt1Btbk5/tch2d4FX\n28/OTuD1173HMxaR2+LKSxhfi9uHw+y/T3m541MqL3f+XrfO8X1YLFnACoyokJ25Kifk8DBw333B\nHuY5cxyndmur8/I4cMBZt3Wr83Jpb0/d/o031I7Wxx5LX6dyynq9/FesAGIx5//x+OiYwr4cxyKd\nncDnPucsqvtAFRDgx9CQ8xuICUqQXt+yc91iMcFUFSmGJW8mKZ3jU85UFrZoOenLK/vabcs+++zU\nonQ6Z+jChdE7YbPh2B2LyEUEVaajqBI2TX8b22zJwsFMUnl/yUe55E1g6ByfqoJzwpchP6yqB1c+\nppxsJ14IkyerXxbjx0cfaVSM0UuFxoEDqXW9/HxamQgPr99GTE5ssyVLEiswck0Q57PbKS63HRUP\nrl+WcDzuLTBU6zOJNCrG6KVCwx3R5ldHS36hh1l0v40Yh222ZEliBUauUWkIXjNEOTNX7uJG5BTu\nW7LEP0JGVR5EPPi2EF1hIL/8VYJcpWWoXuiZCAq36VMnhEQp9bCVCSxFixUYuWTnztGXu66QXpAH\nv7xcrz2YvjBsIbrCQH75q/Ip3JMM+YUeNKTWpFfHiSfqKx3LpdTt/ZIRba+2cd2P65huJq77cR23\nvdrmuT7fBBEY5GxfGjQ2NvKOHTtye9LTThst2xCPA9dcA3z3u8AVVwAPPADMmwfs2hXsmGVlo9Ey\nsVh6xJWgoSE9QqmzE/jkJ4GjR0fXVVYCb70FnHBCsHFYwiP/DkTOa1mF/Bu2tgL33utEq4l7iRlY\ntcr/fOPGAUuXAnfdlTqGT3wCOHYs9Z7SIcZp75fQtO9px9JHl6JvoG9kXVWsCs2zmrF+9/q09Wsu\nWoOmmU35GOoIRPQyMzeabGvDajNh165RYQE4D/q6dcCNN47WkHLH6Tc0+B9XfrB1wqK1dfRFI4dH\netVqsuQO+XeIxVLDkOVF/g3lHIxEAli71lkE27ap82cAYHDQ2X/37tR7Qdw/KmEhh0e3tIyGTdv7\nJTTLty1PEQoA0DfQhzUvr1GuX75teS6HlzFWw8gEWbsQxGLOwzk0pJ+pdXYCn/oU0Jd6A2n51a+A\nCy90ZooC+ditrcA99wDXXuvkUag0GpU2YskOYbQ8WbsQiMKS4mU/eTJQV6fXWImAk08G9u0D/uAP\ngA8+8C9A2dAAbNlitdKIKLulDAzzdyqBMHxTwNybiLEaRi7QZT4PDPgnUV1/vbmwAIDLL0/XNIaG\ngGXLnCSwtWudl8q6dU5Sn9dMtkRo39OO6bdPR9ktZZh++3S072n33ylXhNHyVImUw8Opx/ngA+BH\nP3K0ARXMTvImM/D++3phIVcA3rnTaqURUjupNmvbF8I9bwVGWOTMZ0E8PlqiARg1UcmZtJ2dwM9/\nHuxcH3yQ/kAnEsAvfuGUJpfLXI+Bh1zYiTu6O8BgdHR3YOmjS9H6WCum3z4ddAth3K3jQLdQfh6s\nIFn0wpzoFvQ609P55wPPPJPZ+Nz3SdCsf4uWlXNXoipWZbRtVawKK+euNNpWd8/n+t62AiMsuofM\nXUPK/XAuW6Z3gOooKxs1Twi784EDQE+yjYgQJioBVYLo7MSrd6xGR3cHAGCInd+ho7sDizYtwtTb\npubu4QpSX0rXM0XXT2NoyDEfyROToLiFQZT1sMY4TTObsOaiNaibVAcCoZzUv1M5lQdyeOvu+Vz7\nQKzACMvOnaO1nYR6r3Joyw+nqv4T4ES4eL0AZNOE7FhXOcTzoGXkWlXe371fud7LdtzV35WXGZkn\ncsFKt6DfuVNdfBIAHn00fWJiimjg5G7UZNES9P5umtmEd775DjYu2DgycXEzzMOBoqN097xufbaw\nAiMT3LNDlRCRZ2orVqgf9MHB9PVCk1CZJgYHnYq4qsiXHJsS8qEqB7UTC/oG+tD8UHPhCA13wUq3\noP/976M5T1VVqs9CPn/YjpBjhLD3t9hPh+k9LISVbjIU9lkIS6QCg4imENFDRNRLRB1EdJVmu5uJ\naICIeqTlk9LnDUT0MhH1Jf81iEXNMbrZoddD+MIL6mOp2qyKF7/KNCE71mXq63NuSsiHqqyyE5Nh\nB98hHioMTUMVRuvWMt56C6ioyPxcfX2OKVR1fpV2Yxkh7P2t2k9g6ruQhVUmx4mSqDWMuwAkABwP\noAnAKiLS6NV4gJmrpeUtACDkoB3aAAAgAElEQVSiOIDNANoATAawHsDm5PrCQTU79HsIhQYyfnzq\nepWm8Bd/4Wy/cyewZMloH+14HJg6VT2m11/P+YOfbVVZZQ5w24nrJtXh2sZrES83u0UKIv5dFZk0\nOAiccUbq5CNouXPZ3yXT1pZ6b/hpNxYA4e9vr8/dvgudyctL6NRNqstL0l9kAoOIJgC4DMD3mLmH\nmZ8F8AiAxQEPdR6AcQBuZ+ZjzPwTAATgC1GNNWN0s8Mbb/R/CFes8I+NB4BHHnGSsM4+2/F7CEd5\nIgH09jqhtm5isZw/+DqVOApV2cscIOzEwzcN451vvoPZtbMRJKco17bfNHSaY2fn6G+oc3wLGhqc\nyYSMOxRXIN+PftqN9W2MEPb+1n1eN6kuTVjo7nHdPUogvPPNd/KSIR6lhnEygEFm3iet2w2nR7eK\ni4joMBHtJSI5sLwewKuc+vS/6nGc3KObHba1qR9C+QF85hnzKKmLLwZ+8xt15NUjj6Rvn4dQSJV5\nKFNVWcy4Fm1aZGwOWL5tOQaGNVnxCnJt+03DHZl04MCo+UncN7roJbFs2aIOoojHnS6LbnPW6tXA\nq6/6511Y38YIYe9v0/28TF7ZnIyFJUqBUQ3giGtdN4CJim0fBPBpANMA/CWA7xPRldJxug2PAyJa\nSkQ7iGjHwYMHw449GKZ+BfEQyg+gu62qFx1q2yUSifTzi2SsHIdCqsxDmajKfnZbQK0deGkMUQu0\nrBDGRKQLohA5Om6hMDwMXHWVPiT86acdjdb6NkYIe3+b7qe7bzu6O9DR3ZHmm8v3vRtZaRAi+gyA\n55i5Slr3LQDnMfNFPvsuA3AWM19GRH8D4IvMPF/6/FEATzHzj7yOk5PSIJ2do4UFTzhh9O+urvQy\nIYDjiP79752yC5WVzqzvgw/Cn7+yEli4ELj//tSHXhSrk4vPFSHTb5/uKSwAJ4Z9/aXrUx4+3X51\nk+qwcu5KLN+2HPu796N2Ui1Wzl2Z94JvKYQpJeK3j6psDeD4wg4ccDQUsb/Y79ZbnUKHolBhidxT\nhYzJ/U4gMHjkXo763s1XaZB9AMYR0Qxp3SwAirs2DQZGROleAKcTkSxaTzc8TvZxq+vi73PPVZsN\n5sxJnTlmknAljvHYYyWbmWviW1BFOq2cuxKxstTM+1hZbOQBk/0dBSUsgOClOTo7HU3VrV3ITvM5\ncxzhcOKJjnlKhGbHYs42bn/bsmWjhQ7HWCJoPjHJDBfCohDu3cgEBjP3AtgE4FYimkBEswFcDGCj\ne1siupiIJpPDnwD4BpzIKAB4CsAQgG8Q0Xgiui65/tdRjTU07iio3bvT/5adhSrnYm/vaEy8bLeW\nIY8Q0UQCOOmkks3MnVI5xWg7lS+DXNfN/XfBErQ0h4jIcyduCqf5smXOfcfs/L1xY+o9qFrX1lYw\niaClgGmyn9t0pSPvQRpJog6rbQVQCeB/ANwPoIWZ9xLROUTUI213BYDfAfgIwAYAP2Tm9QDAzAkA\nlwBYAuBDAF8FcElyfX5x25mbmtL/dmsffs5FlQ1aZyZ051mUWDRL+552fJT4yHh7+SFavm05EkOp\nt0hiKJH/8FkTgpTmEJMQwDEl7doFfPazjtYgwrXb2lLvK1XUlOq+1CWCPv10Sd1nYTAVAO172jH1\ntqlYtGlRSuTT4k2LtbXNZA24blKd8rh5D9JIEqnAYObDzHwJM09g5lpm/tfk+u3MXC1tdyUz1yTz\nL05Nhs7Kx9nJzGcycyUzn8HM+Z86q7SFvXvT/5a1jQ0bvGeOL7yg73fhJh4HGhtTH9wSi2ZRvfS9\nkB+iQimdkHVUk5bf/AZ45ZXUIpSm95UOuVfGnDkldZ8FxTTbW2zX1d+VdgyRqe1X2ywbUYdRYkuD\nmBIkiUo8yP39o4UC58xxZoNz5jiVSQEnLNI0k1dEvogHtwQzdYO+3HsSPSMzPp0pq1BmZpGgm7QI\ngib5lZenV1wWiIlNCd5nQTHN9vZKtHPT1d+FxZsWo/Wx1pT1UUcdRo0VGKb4JVHJuLUN0YHPy2Ql\nz+hUvo2KCsf/IR+zxDJ1g77cu/q7RmZ8qlldIc3MIiFM5rcXbk1E7pOh6pVRIvdZULxCX2UtIeiE\nh8FYvWO1p4mqEBzdMlZgmCLbmeUXemVlahSKm6Gh0UKBbpOVLttW9WKQcy+8kgSLmJVzVxqX9/Cj\nprKmoGZmkRBk0hIGt0AwqXc1BvCayMimKdOADRkGh/Kz5auZkhUYYXDPun7xC/2DrOuRITvM5fUr\nVvh3X/NKEiximmY2pYXGhqU6Xl1awgJInbS0tKhrRply8snp69yRWbYTHwDgU1M+pf1MVECmW0ip\n5ZoQVDPJZzMlKzCCopp19fWNqvK6HgYyiYRTKFDnEHdHzei6r6n2LRDCzIDa97Sjd6DXc5u6SXWo\nqazxPVZHd0fhtW6NkhdeyMw8FYv5R2bZTnxo39OOX7/tHdGv63kBOBOXlsYWlJH+VSs0GNNnJp/N\nlKzACIrXrEvX51tFLDbqs3A/sO5wWZ0pQjTDKbA8jLAzIL8bXvgkFtYvNBpHvtpY5gRVKK6u17cK\nk8rGthMflm9b7tmYy4+eRA9++spPMcxq4S7u6SDPTD4jAq3A8MPk5S1mXao+3zr8ErPcjZmK6MEN\nOwPyu+H7Bvpw/dbrseXNLcZjKYhS5lGhyruR1+n6rbghGu0QKSebllheTxRE8RIeHB5Urpf9bEGe\nmXwWJbQCw437odF11ZOd3rt2AccdB2zbpvdlXH11sMSsIg5j1NXG8auZY3LDd/V3+R7HTcnkYqjy\nbuR1O3eaaRkiMfT99526ZNu3j5YLGcP5Fiqy+RLuH+wf+X8QrSGfuRpWYLiRH0CvrnrCLDU4CHz+\n885D51WKYuNGs5d/CYQxejW+98Kkrk4YSiIXQ3Uv6krVBGHfvtESIiKar0gnKtlA192xpbHFyJfm\nhaxBBNEa8pmrYQWGjPsBVOU6uJ3eAwOO05sZeOMN/bHl/XVqf4mEMeqcgF7OQSD1QYiKksnFUE0k\nVFnfqlIzpoh9i3Sikg1UL+eNCzbi7gvvNvaleSE0iKBaQ75yNSIrb14IZFzevLUVuPde50UdjzsP\njvwA6kqLm9LQAHzuc8A99wDXXptaNlpUID10KDWZqghLTHuVGn/nm+8YH2fqbVNDhyoSCLWTajF/\nxnxseXNL4ZY2N0FVylyYROV1OhoaHHNVc7NTrsYEUfKcObWcvwXAaGCHaWa3DrlUf/ue9ryU4Q9S\n3twKDIHqoXQTjwMTJzq9L0wgcmzK4mUvn8Pd76C11elFoEI88EWC6mGqilUFVpvb97TjKw9/JVAn\nPWBUMEU1jrwjT2QEwnEtI69T9dOYMsW8F4uYqDCrJzhjHJM+Fqbk+57MVz+M4sak7EIiAXz842Z5\nEYDzsD39tPocqh7LQHp5hgKOhtIRpY01TIlyocbnM149UlSReaqJnrxOZVYKkugnqtQGCcAYQ1FW\nUQZSFNM9aQWGwDTXYcuWVD+DH+ee6/zr5Z9wC5Jly4r+wTOxsaoSlUR5aLqFsGjTokDVawFgQmzC\nyLlKpoKtV/9vMcFYsiR1H7f/q7PTqUUGOPuKUugqTjlF3fzLz69RxNWTgyaaegVS1FTWYEJsQqDz\nF8s9aQWGwDTXIWgBOJFrodpvcHC02Y27mc327UX54JmiSlT6ysNfwdUPXx3abwEgRaPIZ7x6VlFN\nMNoVLzh37xW5k55X+fNx44IHYBRxOHiYRFNd3bNYWQx3zLsDPf9vD/gm9myKJFMs96QVGEExKQAn\nGh0dOODkZ4ikKvd+AwNOHSpV5jhz0T14QVCZiwaGB7RJTjJ1k+qMGs3MnzE/7YEt+qgp1Yvc3TBJ\n4C5RLvaR65KpeP311AhBgZeWUcTh4GFMl00zmzAxPjFt/cDwQMp+JoIgXh4PfU/mughhpAKDiKYQ\n0UNE1EtEHUR0lWa7G4joNSL6iIjeJqIbXJ+/Q0T9RNSTXJ6IcpwZIWsiDQ3qbUTZBVlF37JF3c97\n6lS9ACqyBy8IYVVw8cJXzfDkB699TzvW716fUtaBQGie1VxcDm83utI0KkSwhJ9WLHwb4t9YLFjf\n+CIPBw9rujzcf9h3v/kz5vuef2J8Yqh7Mh9FCKPWMO4CkABwPIAmAKuISFWNj+C0YJ0M4AIA1xHR\nFa5tLkp25Ktm5vMjHmc0bNni2Hmbm1MdikSjpiahol9/vf7BVtmmgaJ78IIQRgWvm1SH5lnNWL5t\nudK/kRhK4Ln9zwFQzxoZHKisSEFi6muTzal+WrEQJrLJSvSe1zX/kinyqrZhTZd++7XvacfPXvmZ\n7/l1gsePfAR1RCYwiGgCgMsAfI+Ze5j5WQCPAFjs3paZb2PmV5h5kJnfALAZwOyoxpIzVqxwfA0i\nQ1YwPJy6bmgIeOQR9THeeEOdQS4oogcvCEGyumNlMbQtaMPKuSuxfvd6z3BG0ZCmZBzebsLUFRP7\nHDgAnHiiM6E5/nhvx7ecHKhq/iVT5FVtgybNicAM1X0o77d823KjkHCd4PEzN+XjHo9SwzgZwCAz\n75PW7QbgWe+bnLjJcwDsdX3UTkQHiegJIprlsf9SItpBRDsOHjwYduzBEWo4s1pzGBpKVdG9Znji\nISzyBy8IIvTWr7xCTWUN1l2yTlugzY1oSFOyDu9MWLZsNGT7/feBY8f027rDauXmX26Nt8iKY7oJ\nEgYucoNUgRnupl0mL+5YWUwpmEzMTfm4x6MUGNUAjrjWdQNI9wylcnNyHHIRnCYA0wHUAXgSwC+J\n6GOqnZl5DTM3MnPjtGnTQgw7JEGjpWIxoEbzchQCQZ4Jzpkz+nAXyYMXlKaZTaiOVys/q5tUB76J\ncejbh0YeQNNEqY7uDnR0d5SewzsTRK0oP+RWwXJYraBENV7TUhteWoO7aZdJBz7hJHdrDybmpnwU\nIYxSYPQAOM617jgAH+l2IKLr4PgyLmTmkekOMz/HzP3M3MfM/wDgQzhaSGHgdvKZIJL+TEN3izSe\nPSimanXrY62Bj80YDWvMZYG2gmTZMrMJji6ySv68RP1qJnhpDfJn7XvaceSYe/6sRoSUm/QHl9fn\nowhhlAJjH4BxRDRDWjcL6aYmAAARfRXAMgBzmfldn2MzYBjQnAtMtItYLNVOXFmpdhi6KeJ49jCY\nqtVrXl4T6vgMHikVMmaFxa5dZjWkhOPcL7KqRLUME7zMPfJnpv4LwcDwAL7+6Nd9z+Nen+sihJEJ\nDGbuBbAJwK1ENIGIZgO4GMBG97ZE1ATgBwC+yMxvuT6rJaLZRBQnoopkyO1UAM9FNdaMMcnFGBhw\n7MQiQcr0ISviePYwmKjV7XvafSvdelH0ju5MWbTI+3PRq8WvPasgkQDWry/5yYyKlXNXKvvOu3Mp\nwtxzvQO9I1pGPnteeBF1WG0rgEoA/wPgfgAtzLyXiM4hoh5pu78HUAPgJSnXYnXys4kAVgH4AMB/\nwwm7ncfM4dN/o0bna5DDY0XOhRyq6KcxFHk8exj81Grh/POijMrQtqBNW47BxJZcspi0Dd68OX2f\n445Lv69bWpzw8fp6oL+/5CczKppmNmHdJetSgjVqKmuw9uK1KbP7sI5n4aPIZ88LL2y12kxobVVX\n8tSVkSZyPtM1uVFVJS3C8uZ+BCnjbFoVtAxlGIbahFJTWYND3z6U0ZiLFnGPeplQp04F5AhD1X2t\nquasqohbgrjvV5OS+WHLnxMIwzcFCKaJAFutNhfofA2dneq6PoAzS3v0Uf0xx0BYbZDs1PY97caR\nUTphAThtXXNZPqFgkO9RL3p7R3t7i659Xl0mBWPAZKq6X1ftWOV7/zbNbELzrObA5yv0sG8rMMKi\n8zUsW6bOyxAmqr6+0faa7oq0RR7PboJpdmr7nnYs2eSqwJoB8gO+eNNi0C1UXMIjTOnwFSvMOvCJ\nbn0iQc+vy6RgDJhMTXJ/dPfv+t3rA52rEHwUfliBEQYvX8Njj6n3cbe/HEOhszI6jUGsF9mtizYt\n8tQaMkHUl8pF7Z3I8LpfdMLkhRe8q9IKEonRxLy9e9Pv6+uv1yf5lbiWYeq83t+9PyUze/GmxYHN\nUYXgo/DDCowweKnnH/+4976JhOOnWLVqzITOypSTogBjcr2s/ueKomhe4xdqrRMmuoKXAtFLQziz\nVYiyNjpfZ4mZTN2YmoimVE5JMV3JRS9NKKfyghcWgBUY4fDyNQizUkuL/mGVZ2slPkOT8QqPHeIh\nI/U/G3jNInNdPlqJV6i1lzDxM0mJXhpr1+r9HHJZm8rK0SKEcgRVCZlM3ZjUPIuXx3G4/3BG924m\nYeO5xAqMMMi+BjE7a20dfXDEQ2xiPx4DdmDAPzy2blJd3vIlVLNIUWBu0aZFOS0fnYZfqLVJ218d\niYTTj0VltqqqSu/kJ/s6xsgkRxXe2tLYMvJ3TWUNmINrFG50/V0KDSsw3Ojswar1utndihV62/FE\nRWmtEmnL6oWX9iCcffmIEFE5GoVwUxWYy7kJy8v8adr21w3RaLLeCSeot+vrA5YudcLDhTlK9nWM\ngUmOwJ1NffeFd4/8XR2vDpTRraIYnN0CKzDc6OzBqvW62d0zz+i1i48UpbXETK+EZ25e2oNw9qnU\nf9MWl6bEy+OoqazxTIbyM43lVBPyMn96CROvTG05vHvOHL3/wisEfAyYUnXmSHl9UH+b0CSEL69Q\nEvJMsYl7MnJykpyUpFrPrE9k+s53zGr3AE5i3pVXAg88kH7eEkKXgCfqPAlUSX3Lty0P5Qgvp3Kl\nbdgvka/sljJPE4N7zHnjM59xfApuRKc9wDGVrlrlCAVZuFRUAG+/Dcybpz6GCUV2r/oljMqfV8Wq\n0DvQm7J/VawKzbOasX73+lD+ioK5b1zYxL2w6DQG1Xrd7G7ZMn3inopEwtm+xOtHmdbGURVTC9Js\nST62zpHY1d/l6YfwMo0VlPnAL2+ns9NxaAPp92oi4dxn8jHcHR/9KKJ7VZWAt3jT4pEqyO7P3cIC\ncMyRa15e4yssylCW1j64oO6bDLACQ6CzB4vMV/f6Z55Rmwp+8Qu9OWry5NSHc84c4PLLgcHBkq8f\nlWltnMpxlSP/r45XpxWAi5XFUkxNzbOatSG8ADz9EKp+4UB6g5yCx8uXNjzsCBN3VFWQHi9FFFKr\na9krOjSaRuiZRDNNrpyMtRev9b3XCyICLyDj8j2AgkGnMciZr/L6c88FXnstdb0wXek4csR5QE84\nwdFEnnlGvZ2YuZVQ/SjAERq6l63OXKCqyTPMw7jmjGu09XzEPl4Pt58fwm2qjZXFcMe8O4pHWJiU\nBRFahrjPTKowA0VZ30z3e4sOjVH6pQ73H1be6/I9PqVyCj5KfDTSl15E4AEo6HvMahiA83Bt2KDW\nGH7/e/P6Tn4zNDm6xav7WRHN3KLAq76UrpTIlje3aPsAmMwWayfVamd4ql4GojNawaGL6jPRFoaH\ngW3bRv8W5qmGBu/9ilAL9jIziklHNs/lvse7+rtGhIWgGJJIrcAAnIerv3+0NaW89Peb13cymaE9\n/bS++5nIvC3xZCg3XvWlvDqP6V74frNFAuFTUz6VJqQWbVqE6h9Uax3sBdlXQxfVZ6otjFMYGYTg\n8LoHi8h/AThmRl3EndBQg/rJVOh8FaYmr4K8xySswIiyw93Onam9MNzE40Bjo167KLKHMCq8hIJu\n5ucuxSBrJX6zRQbj12//WvkAq5ydgoKrJOpVMVnuZ3HggJN7oeL11/X3vFfjpSLTgptmNuHaxmu1\nfd7dPrYw4dxefjnTKL+Cu8dcRCowiGgKET1ERL1E1EFEV2m2IyL6IRF1JZcfEo3e0UTUQEQvE1Ff\n8l8fHTkDou5wp6tWCzgP2SOPeJdhKKKHMCq82lHqoqsAKLWSRZsWoSfRo3Ray/hl5upeLAWFV8Xk\nZ55x/hXbxZJBAvG40wApnrw+sZi+oKGu8VJNTVFqwXdfeDc2LthoFHgxpXKKsrOeDr82wCYCqCDv\nMRdRaxh3AUgAOB5AE4BVRFSv2G4pgEvg9Pw+HcBFAL4OAEQUB7AZQBuAyQDWA9icXB8tJh3ugpaV\n1lWrFf2S/cZTZA9hFHiF3Oqiqw73H9Yer6u/C8yM6nh16DGJXuCF1O0sBa+oPhHW3damjvJTVaQV\n/TDkagUxzQtTlOgvQnQ9sFU+hiAZ3B3dHZ6RTl4TlIK9xxRElrhHRBPgtFU9jZn3JddtBPDfzLzM\nte3zAO5j5jXJv78G4C+Z+WwiOh/AOgAncXJwRLQfwFJmftxrDIET90w63Om66qlQdSUTCVIiuWnq\nVKBL0222tbWoIk+iJEgXPsC8E19YCjXJagTdvfuJTwBvvDG67pRTnPvPy58RjwMzZgD/+Z+j97ku\nKVBsX2RRUn5EdT9VxaqUL366Ra9h8E35TZ7OV+LeyQAGhbBIshuASsOoT36m2q4ewKucKsle1Rwn\nM/w63AX1b6giU0ToosCr/HmRRZ5EiW7mpyNTJ6WficBvxph3dPeuLCwA528/53ci4Zif5PvcK6Gv\nCKOkVGRS4kOHrplSGalftXJv8GIgSoFRDeCIa103AEW1PVQnP5O3q076MdyfeR0HRLSUiHYQ0Y6D\ncl9iE/wyZYP6N1QP8fCwExkln1PnGB+jTu8wyKaqMAiTkxcF3WBJde/KlWVlFi5U3+dyxWVhflLd\ngyXYntVtgooSOYjjzzf8udMMjNP9lvHyOO6Yd0ek5842UQqMHgDHudYdB0BRbS9t2+MA9CS1iiDH\nATOvYeZGZm6cNm1aqIErMfFvuNmyxcnebm4edSrG445tWEbXp2CMOr2jIGhUS01ljZGWUgyx8SPo\n/GebN+v3MbnPS7DXfDZ7r4ggjtbHWrHt7W3KbcqpHGsvXlvwPgs3UQqMfQDGEdEMad0sAHsV2+5N\nfqbabi+A0+WoKTiOcdVxskeYWdWKFcD27Y6zUfcAqvoUjNH8i0xofawVizctHjElhJkluh3qOoR5\nquBLOOjMnYmEfqJjcp+XYK/5bOU7yJFOa15eo91umIeLTlgAEQoMZu4FsAnArUQ0gYhmA7gYwEbF\n5hsA/C0R/RER/SGAbwG4L/nZUwCGAHyDiMYT0XXJ9b+OaqxGBJ1VCUHAnK49yBneZ55Zcup9rmnf\n047VO1ZnZEoQUVay70RnoiJQfpsomeJu7OUXOguUpPZgQlT5DrGyWEo0nlzzzKs0TaHnW+iIOqy2\nFUAlgP8BcD+AFmbeS0TnEFGPtN09AB4FsAfAawAeS64DMyfghNwuAfAhgK8CuCS5PncEnVV5lWKQ\n+xd0do7JBzRKlm9bnrHduYzK0jQGXT8O97kK3kwVxJzqdZ8HDSkvIrwyv4Mwp25OSomPrv4ufHXz\nV9G+p92z+GWh51vosP0wTOjsBK64wulZoar9rwqndfcK0PXasATGr19FUOLlcUyMT8Th/sOYUjkF\ngKOB1E6q9YyeIZBRCHDOMQkXNz2OaUh5gaMK235u/3MZa6o6aiprsLB+IVbtWJX22dxPzMWvlvwq\n8nOGxfbDiBpdvR75c7d2cfQocOON6m2sCSojxEtdRVmIWzoxlHCS/ZIJW/2D/di4YCPe+eY7npFU\nBWuiisLMFGXJnDyjK245u3Y2Ni5QWcwzp6u/C7NrZ6OlsWVE0yincrQ0thSUsAiK1TD88NIMOjuB\nSy8F9uxxsl/d1NQAhw6ZaSAWY6beNlXZbzteFgcIaVVAwyAS91Tl1b22LxlkLaXIE/X8uj1mKwlU\nl8RXaFgNI0q8NIMVK4Df/MYRFqLSrZzkJEooqDSQwUHgjDOKeuaWL3RlQRLDCV9h4WVXlhFRNKaR\nVIVeZTQQYULKCxiv4pYAMH/G/Kyct+B9XSGwAsMLrwdHbn8JjHYvUwkYlYlgYMA5hjVNBSaTCBOT\njmmAY/YSobTLty3HyrkrPSOpijXqRUmJJep5FbcEgC1vbgl8TNOJRzbL1+QDKzC88Hpw3O0vEwmn\nOqhKwGzdmhqBImshRTxzyxe6YoVRlVmIl8dx5NgRZSitaW/ygiBslFOJhdr6/WZBX+otjS1Yf+l6\no9I0BCos/1aGWIHhhe7BefppR6OQhcnwsJOwp8vBkLEO8IzQVbC9Y94dGTfBmRCbgInxiWmVSkXp\n9OXblqN5VnNhV7IV+AVr6CjBRD05P8Ldm91UWxA8uPfBtHtQN1kRLWBLBev0DoMIN/RrgSloaBh9\n2MawAzxoRdpMzpFNU0BRODNtGDcAKIMW3L+fVyVZHW0L2owr0hIIwzcZvivygHV6Z5sXXtALC9H3\nQjczKzH7sClefbujRGRu802cFtIYLzNrqeKX0FUUzkz5PhvDARZe7X8FYQpYXr/1+rR1Jv4tXVvh\nYsEKjDDoVHYTtb3E7MOmmDy4UdK+px1b3tyCYXYc1esvXW/cEMckkaugo6LcwRoiwGLZMu/9ihC/\nF7BfhBQQrlR+V39X2rn8fCW5mjRlEyswck0J2odNMHlwo0L3YHol/AXFKyoq77NIXZmatraC1jKC\nXje/F7BXH4oyKks5vuzjMMU92XGX3C+n8pFJkTCV5nLSlA3G5XsAlrGBrsxGmHBUP1+I7sHsG+hT\n1oYKildUlNtmLl5iAHLn81BpscCo6bMAE/DCXDe/F/DSR5dqw6iHeAhLH12K5/Y/h/W714cqda6a\n7Iixqr6L7hwFra26sBqGJSdEFY5qotZ7PYCZCgu/qCivl1jONA+hxbo75QEFG8YdZPYtrqMusGF/\n936jfhd9A31Y8/Ka0H0xdJMd3XfRRWMVUw6PFRiWnKALhQ066zZ5sWTzAexJ9Hh+rhNWQrDl1H5d\nRAEWpiZLecKgo3ZSrfGs3TSR002sLKad7OjOPcRDxZPDo8EKDEvOCNq3W0W2nJimdPV3YdGmRaBb\nSKkl6ISVsGfLZN1+XUQBFrrrxuCU62yiOcyfMT/rs/ZrzrhGe//qzi0mSUWRw6PBCgxLUeFX5gHI\nvN+3KSotQWd6081ksyvUJhsAABYKSURBVGq/LqIAi5VzVyJerg57lq+zyfVatWNV1ktyeJUT8TK/\nRjFpyieRCAwimkJEDxFRLxF1ENFVHtveQESvEdFHRPQ2Ed3g+vwdIuonop7k8kQUY7SUBqa+kKaZ\nTVg5d6VRuRCvTF0/3FqCzvSmE15yzapijMuPEq8kYnGdM9EcJsQmhN7XjZfgisr8WohEpWHcBSAB\n4HgATQBWEVG9ZluC001vMoALAFxHRFe4trmImauTy/kRjdFSApg+jMLWrSqD7obBWFi/MPSY3C8P\n1SxSJei8alZlSt5DewOOafm25b55Mvu792dkbuwd6A21nwo/wVXsmoSOjAUGEU0AcBmA7zFzDzM/\nC+ARAItV2zPzbcz8CjMPMvMbADYDmJ3pOCxjB/fDCCDtRWRi65ZRdUYzpXZSrfZlKNYv3rQYleMq\nUVNZMyLodDWrMvVrFGKCmN+YTExNtZNqc2Zu9KLYHNVRknEtKSL6DIDnmLlKWvd3AM5l5ot89iUA\nrwC4h5lXJ9e9A6cveBmAnQBuYObdJmPJWS0pS8GgqxUUNlQyDC2NLWmx/FWxKnzupM/h12//OiWU\nV65jpGs1m2ntIb+GQfkg0yZGqvpd2Wp85EdLYwvuvvDunJ83W+S6llQ1gCOudd0AJhrse3NyDOuk\ndU0ApgOoA/AkgF8S0cd0ByCipUS0g4h2HDx4MMCwLaVA0Jj3qKmprMGWN7cox7Dt7W1pAqFvoA/N\nDzWjfU+7kQM/DLnMqjfFb0wqU5Oo6eU2O/rlYWSbn73ys4Iw8eUDX4FBRE8REWuWZwH0ADjOtdtx\nAD7yOe51cHwZFzLzMbGemZ9j5n5m7mPmfwDwIYBzdMdh5jXM3MjMjdOmTfP7OpYSI0jMeza4Y94d\ngV/EIst4/oz5xnH5QXwS2RJEmeA3JpVvauOCjeCbOMUHYJKHEZSgAQ8DwwNFVc4jSnwFBjOfx8yk\nWf4UwD4A44hohrTbLAB7dcckoq8CWAZgLjO/6zcEwKd8qGXMYhLzDoz2PKiprNGGbwZlXJlTWSdM\njaq+gT5seXNLIAe+iU+ifU+7NrmwJ9GT9ZmxTrDpnP49iZ6RbQFE7pvyY+4n5uLQtw8F9okUUzmP\nKMm4lhQz9xLRJgC3EtE1ABoAXAzg86rtiagJwA8A/Bkzv+X6rBbAxwG8BEeY/TWAqQCey3ScltJk\n5dyVSh+GiHlXRafIPTMyqS01ODyoLHNtitdLR66XVUZlaXkcwrS1eNPikXpaADxrFnX1d2W1rpVJ\nPSjxnaZUTsGRY0dGotjc2+qOFVZY6H7nF959YaSTYpDjF1M5jyiJpIESEU0BsBbAFwF0AVjGzP+a\n/OwcAFuZuTr599sATgJwTDpEGzNfmwzFvR/A/wJwFMAuAN9hZiNPtnV6j00yaczUvqcd12+93ij8\nNmomxCaAwWnCrnlWc+CCeFWxKlSOqwz0Peom1UXWxKp9TzuaH2pWJiiWUzmGeTjlt/Fzgk+9bary\nu5RTeehyHrp9ayprUB2vRkd3h9HxY2UxrLtkXcmEygZxetuOe5YxT6YO1ExeYrk4nhcm3QP9BLIq\nUs3vfIs2LdJu07agzfPzbEfBeQnfMirDhks3lIywAGzHPYslEJnao6N2sOdKWAD+eR8m/pMgfgVx\nPl0UWzmV+zqUm2c1+3ZF1B3bdIwAlAEJpSYsgmIFhmXMk6k92q/8R1SUUzkIpH3x1VTWhBJcXgLT\npFx7UO2so7vDs0+FnwBfv3s9rm28diTowISqWBWWnrnU+Pp09XelhGeXUnmPTLACw1L0ZFoGI9Pq\ntvNnzB8x2WSTIR5C7aRa5YuvKlaFO+bdgTUXrQkcJuolME3KtUdJ3aQ6XwEuIszuu+Q+4+M2z2rG\n3RfeHVg7EdpjVL6eYscKDEtRE0UZjEzLTdy7896R82ebju4OrN6xGp876XPKcNymmU2ojlcbH8+v\nzEWQcu2ZQiB0dHc4obY+r6b93fvRNLPJ+DcT1WW3vLkl8O9UbG1Us4kVGJaiJqo+yaI+ldfsU/dZ\nYkjRDjUkuh7UMgzGtre3Yf6M+cridn6aThAzS9By7ZkgXuRd/V0oKyvzrC4rBJmpdiiuSVgtcKzm\nXbixAsNS1ERdBsMrEXDjgo2BS46YCACZYTavIbVqxyplIyev78A3MQa/P5iWQa0jaLn2qBgcHkT/\nYD+AdEEta0Wm2qG4Jl7mLgJptbMplVMw9bapoFsIdAth6m1Tx2R5ECswLEVN1GUw/JrfBHmh102q\nw4ZLN0SWWa7DbYbLpH+68AfRLYRxt44D3UJYvm05Vs5dmaLNzJ8xP9AYq2JVgUxlwKjwlE1INZU1\naVqR0A51QoNAI9/dSyNhMMaXj1dmpH949MOUMNuu/i585eGvjDmhYQWGpajRvQDClsHw67cRRBAJ\nO/vai9cGHkdQ+gb6RrLOwzbwcddpEmYnlV/Iq+Ocm3Iqx5qL1mD1/7M6Y+EptA4VOq2SwSnfvXJc\npfYYh/sPp127WFlMaYIbizWlbOKepejRZWubJKW5j+OXMR4kSU1kOE+pnILD/Ydz4hTPpPS2X4is\nXB5dV5pdhSjXHlVWva5Mu1/2uMlv5z52+552zyTCTEvRFwI2cc8yptBFBgVxfptGW4nZu4lvYoiH\nwGB09XflRFgAwOodq0ObSfxCZOUZfBBNSzSYMu2A6EdHd4fyO/qZ4vwSDFVmO7/7Z6zVlLICw1IS\nZOr8DhJt1TSzCZMrJmuPlateHCoYHNpM4jduubOgaf5FVawK82fMR/NDzZGG4QphLufgLN+2HM2z\nmrWmOK97QWe289onVhYbc533Mq5Wa7EUArWTapUvMdMZYFCBc7j/sHI9gQI5xrNB2Agxr1DZeHkc\n82fMD1TRtaayBseGjmXU/laH8Nn0D/anVLRdv3u91gypu0e8OhHq9iFQSRUgNMVqGJaSIJPIIEDf\n00K33is6K99mijDnb9/T7qlhMDMe3PtgIC2hq79L25vDTbwsuDNclO+Q8TJDhrlHdPtsXLBxzAkL\nwAoMS4kQNjIoLF4vn0xLjaioKK8wKmnhZyZxl1H58w1/jrJbyrBo0yJPDWNgeCCrJeBPnHgi+CaO\nJL9Dp2GFuUdyfV8VOjZKymKBPurHKwrGK6pK/kxoKbnquaHrcxEkwisf1FTWBLpGuu1Ff4sw/VHG\nInnph5FsonQvgPMBHAJwo2iipNj2ZgDLkdpE6XTRgY+IGpLH+jSA/wTwNWbe5TcGKzAsYfELyYwC\nuiV3nYZFSDEAz859xUpNZQ3umHdHmgCMl8fBzBgYHhhZFzS8eqyRr7DauwAkABwPoAnAqmQHPR0P\nMHO1tAhhEQewGUAbgMkA1gPYnFxvsWSFTH0gfviFukYdWdU30IclDy3BVx7+ykiocKkIC1GZV2Uu\nmhifmCIsAFs8MEoiERhENAHAZQC+x8w9zPwsgEcALA5xuPPgRG/dzszHmPknAAjAF6IYq8WiwtRW\nHbaUut8LS5TRDlqa3IthHk57eWabID0qTGlpbNH+LqIsyPBNw1g5d6XWpGWLB0ZDVL/uyQAGmXmf\ntG43gHM99rmIiA4D6ATw/zGziL2rB/Aqp9rKXk2ufzyi8VosaYjy4DrcPgCR3Cf29cLkhdU30Iej\ng0cRK4vl/EUfFYPDg6iOV6N/oD8SjaZuUp1R5rr4bXTkO3KtVIjKJFUN4IhrXTeAiZrtH4Tjn5gG\n4C8BfJ+IrpSO1W16LCJaSkQ7iGjHwYMHw4zdYjEik1Lqpi+sYR4GEXlmksfKYkbHyhe9iV4Mfn8w\nVBtVmSAmQa8s7ihNi2MdI4FBRE8REWuWZwH0ADjOtdtxAD5SHY+ZX2fmA8w8xMzPA7gDwP9Ofhz0\nWGuYuZGZG6dNm2bydSyWUGSSTR4k1DYxlMDkisna7Qtd+zApJe5H0PBVr9/AOryjw0hgMPN5zEya\n5U8B7AMwjohmSLvNArDXcBwMjExH9gI4nYjk6cnpAY5lsWSFTEqpu30kNZU1npVbu/q7PKuq5oIw\njnh5Nh80H6VuUh3aFrQZ9+qQ8eoBYoVFdERikmLmXgCbANxKRBOIaDaAiwFsVG1PRBcT0WRy+BMA\n34ATGQUATwEYAvANIhpPRNcl1/86irFaLGHJNJJKOGg3LtiI6ni1b6e+qPI2/F78BEoTXlWxKmXv\ncL/zuB3SspDUjaOmssZXSPgFG2Q7ys3iEGVYbSuASgD/A+B+AC3MvBcAiOgcIpJrBFwB4HdwzEwb\nAPyQmdcDADMnAFwCYAmADwF8FcAlyfUWS96IIuvX3XMiF5g4n9devDbte9194d0juRx+VMWqsP7S\n9WnXQo5i0tXY0tXlEugqCbc+1mpceNASDTbT22LJIboEwXIqz1uehF9yokl12rYFbb4RZs0PNSu/\nY9jzEyglO98m6IXD9sOwWAoI2Zyie/EO83DW+2SrIBA6ujs8c0r8fBF+vUGEhqASFiZmI69OejI2\nQS/7WIFhsWQRtzlFh6h55OczKKOyNF9AvDyOlsaWwE5qeYauaxgFjJridEmFwzys3RfQh7y6fR46\ngrbFtWQPKzAslizi1+UNGJ1lyz4SHcyM9ZeuT7HVr714Le6+8G6sv3S9kZO6blId6ibVaWfoKgdz\n08wm3DHvDq3Q8Jrde2lVJuYjlSDV5XjYBL3sYn0YFksW8ep9TSBtNdWwxRD9+mYTCNc2XovVO1Zr\nxxUvj6dFcE2ITUBiKOGZA6Kq7Nu+px2LNy1WnitIYUd3ZeD5M+Zj/e71KcLY+jDCkZdqtYWAFRiW\nQiOTF7+7EmuQF6J4weYyGkv1nbwc1pk2IfIqL28xxwoMi6VAyOTFH8ULMUj/7UzQfScvDYtvKp13\nTzETRGDYnt4WSxYRL9AwL36/Yogm5MIJrGvYBHj30bYUH9bpbbFkkVybTdwOa11P8iioilWhbUGb\nZ4Z2phnYYcvJW7KD1TAsliyRSTn0qM4XK4spndgy7gS4WFkM48eNR0+iR7uP6Hjn9z0y0bByff0s\n/lgfhsWSJXLR9tXkfCIUVhU5FS+P42uf+Rq2vLkl7YWeSXZ2FOT6+o1VrA/DYikAMimHHuX5Dvcf\nxvBNw2kht35aQtPMJizepG6amQvfSK6vn8UfKzAsliyhc/hmK7nM73xhnOi5/g6Fcm6LGuv0tliy\nRK5LbmfjfPksG25LlhceVmBYLFkiinLo+T5frr9DoZzbosY6vS0Wi2UMY8ubWyyW0NjcB4uOSAQG\nEU0hooeIqJeIOojoKo9ttxJRj7QkiGiP9Pk7RNQvff5EFGO0WCz+6LrbmQoNK2xKm6g0jLsAJAAc\nD6AJwCoiqldtyMzzmLlaLACeB/Bvrs0ukrY5P6IxWiwWH1Tl2E0bE2UqbCyFT8YCg4gmALgMwPeY\nuYeZnwXwCAB1AHfqvtMBnAOnr7fFYskzmeQ+ZCJsLMVBFBrGyQAGmXmftG43AKWG4WIJgO3M/I5r\nfTsRHSSiJ4holtcBiGgpEe0goh0HDx4MNHCLxZKKLsfBJPfBJtqVPlEIjGoAR1zrugFMNNh3CYD7\nXOuaAEwHUAfgSQC/JKKP6Q7AzGuYuZGZG6dNm2Y6ZovFoiCT3IdMhI2lOPAVGET0FBGxZnkWQA+A\n41y7HQfgI5/j/imAEwD8XF7PzM8xcz8z9zHzPwD4EI7ZymKxZJlMch9sol3p41sahJnP8/o86cMY\nR0QzmPnN5OpZAPb6HLoZwCZm1pfETA4B0DTwtVgskRO2D0cmlWktxUEkiXtE9H/gvNivAdAAYAuA\nzzOzUmgQUSWA9wBcysy/ltbXAvg4gJfgaD9/DeDbAE5lZnWTYgmbuGexWCzByEfiXiuASgD/A+B+\nAC1CWBDROUTk1iIugWNqetK1fiKAVQA+APDfAC4AMM9EWFgsFoslu9jSIBaLxTKGsaVBLBaLxRI5\nVmBYLBaLxQgrMCwWi8ViREn5MIjoIID0Fl25ZyqAQ/keRACKabzFNFaguMZbTGMF7Hijoo6ZjbKe\nS0pgFApEtMPUiVQIFNN4i2msQHGNt5jGCtjx5gNrkrJYLBaLEVZgWCwWi8UIKzCyw5p8DyAgxTTe\nYhorUFzjLaaxAna8Ocf6MCwWi8VihNUwLBaLxWKEFRgWi8ViMcIKjAggouuSXf+OEdF9Btv/DRG9\nR0RHiGgtEY3PwTDFuacQ0UNE1EtEHUR0lce2NxPRABH1SMsnC2F85PBDIupKLj8kopyXwQ8w3pxf\nS8UYjO/TfN6j0hiMxktEVxPRkOvanpe7kQJENJ6I7k3eAx8R0S4imuexfd6vbxiswIiGAwD+HsBa\nvw2J6EsAlgGYC6er4CcB3JLV0aVyF4AEgOPhdDdcRURe7XQfYOZqaXmrQMa3FE7V41kATgdwEYCv\nZ3lsKoJcz1xfSzdG92kB3KMC4+cKwAuua/tUdoeWxjgA/wXgXACTAHwXwINENN29YQFd38BYgREB\nzLyJmR8GYFKGvRnAvcy8l5k/ALACwNXZHJ8g2ezqMgDfY+YeZn4WwCMAFufi/H4EHF8zgB8x87vM\n/N8AfoQcXUdBoV9PNwHu07zdozIBn6u8wsy9zHwzM7/DzMPM/AsAbwM4U7F5QVzfMFiBkXvqAeyW\n/t4N4HgiqsnBuU8GMMjM+1zn99IwLiKiw0S0l4hasju8QONTXUev75ENgl7PXF7LTMjnPRqWzxDR\nISLaR0TfIyLfbqLZhIiOh3N/qJrIFeP1BWAFRj6oBtAt/S3+PzFH5z7iWtftce4HAXwawDQAfwng\n+0R0ZfaGF2h8qutYnWM/RpDx5vpaZkI+79EwPAPgNAB/AEfjuxLADfkaDBHFALQDWM/Mv1VsUmzX\ndwQrMHwgoqeIiDXLsyEO2QPgOOlv8f+PcjBW97nF+ZXnZubXmfkAMw8x8/MA7gDwvzMdpwdBxqe6\njj2c28Qi4/Hm4VpmQtbu0WzAzG8x89tJU9AeALciT9eWiMoAbITj17pOs1lRXV8ZKzB8YObzmJk0\ny5+GOOReOI5awSwA70fRhtZgrPsAjCOiGa7zK3uvq04BIJsz+CDjU11H0+8RFZlcz2xfy0zI2j2a\nI/JybZPa7b1wAiAuY+YBzaZFe32twIgAIhpHRBUAygGUE1GFhw11A4CvEdEfE9HH4ERT3JeLcTJz\nL4BNAG4loglENBvAxXBmRGkQ0cVENDkZwvonAL4BYHOBjG8DgL8loj8ioj8E8C3k6DoKgow319dS\nRYD7NG/3qIzpeIloXtJnACI6FcD3kONrm2QVHLPjRczc77FdQVzfUDCzXTJcANwMZ1YjLzcnP6uF\no4LWStv/LYD34di/1wEYn8OxTgHwMIBeAPsBXCV9dg4cs474+344ESo9AH4L4Bv5Gp9ibATgNgCH\nk8ttSJa6yfFvbzrenF9L0/u00O7RoOMF8M/JsfYCeAuOSSqW47HWJcd3NDk2sTQV6vUNs9haUhaL\nxWIxwpqkLBaLxWKEFRgWi8ViMcIKDIvFYrEYYQWGxWKxWIywAsNisVgsRliBYbFYLBYjrMCwWCwW\nixFWYFgsFovFCCswLBaLxWLE/wVaLdkNvjnwXQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc1676eb710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(X_moons[y_moons == 1, 0], X_moons[y_moons == 1, 1], 'go', label=\"Positive\")\n",
    "plt.plot(X_moons[y_moons == 0, 0], X_moons[y_moons == 0, 1], 'r^', label=\"Negative\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must not forget to add an extra bias feature ($x_0 = 1$) to every instance. For this, we just need to add a column full of 1s on the left of the input matrix $\\mathbf{X}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_moons_with_bias = np.c_[np.ones((m, 1)), X_moons]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        , -0.05146968,  0.44419863],\n",
       "       [ 1.        ,  1.03201691, -0.41974116],\n",
       "       [ 1.        ,  0.86789186, -0.25482711],\n",
       "       [ 1.        ,  0.288851  , -0.44866862],\n",
       "       [ 1.        , -0.83343911,  0.53505665]])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_moons_with_bias[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good. Now let's reshape `y_train` to make it a column vector (i.e. a 2D array with a single column):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_moons_column_vector = y_moons.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's split the data into a training set and a test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_ratio = 0.2\n",
    "test_size = int(m * test_ratio)\n",
    "X_train = X_moons_with_bias[:-test_size]\n",
    "X_test = X_moons_with_bias[-test_size:]\n",
    "y_train = y_moons_column_vector[:-test_size]\n",
    "y_test = y_moons_column_vector[-test_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now let's create a small function to generate training batches. In this implementation we will just pick random instances from the training set for each batch. This means that a single batch may contain the same instance multiple times, and also a single epoch may not cover all the training instances (in fact it will generally cover only about two thirds of the instances). However, in practice this is not an issue and it simplifies the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_batch(X_train, y_train, batch_size):\n",
    "    rnd_indices = np.random.randint(0, len(X_train), batch_size)\n",
    "    X_batch = X_train[rnd_indices]\n",
    "    y_batch = y_train[rnd_indices]\n",
    "    return X_batch, y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a small batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  1.93189866,  0.13158788],\n",
       "       [ 1.        ,  1.07172763,  0.13482039],\n",
       "       [ 1.        , -1.01148674, -0.04686381],\n",
       "       [ 1.        ,  0.02201868,  0.19079139],\n",
       "       [ 1.        , -0.98941204,  0.02473116]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_batch, y_batch = random_batch(X_train, y_train, 5)\n",
    "X_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0]])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now that the data is ready to be fed to the model, we need to build that model. Let's start with a simple implementation, then we will add all the bells and whistles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's reset the default graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The _moons_ dataset has two input features, since each instance is a point on a plane (i.e., 2-Dimensional):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_inputs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's build the Logistic Regression model. As we saw in chapter 4, this model first computes a weighted sum of the inputs (just like the Linear Regression model), and then it applies the sigmoid function to the result, which gives us the estimated probability for the positive class:\n",
    "\n",
    "$\\hat{p} = h_\\mathbf{\\theta}(\\mathbf{x}) = \\sigma(\\mathbf{\\theta}^T \\cdot \\mathbf{x})$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that $\\mathbf{\\theta}$ is the parameter vector, containing the bias term $\\theta_0$ and the weights $\\theta_1, \\theta_2, \\dots, \\theta_n$. The input vector $\\mathbf{x}$ contains a constant term $x_0 = 1$, as well as all the input features $x_1, x_2, \\dots, x_n$.\n",
    "\n",
    "Since we want to be able to make predictions for multiple instances at a time, we will use an input matrix $\\mathbf{X}$ rather than a single input vector. The $i^{th}$ row will contain the transpose of the $i^{th}$ input vector $(\\mathbf{x}^{(i)})^T$. It is then possible to estimate the probability that each instance belongs to the positive class using the following equation:\n",
    "\n",
    "$ \\hat{\\mathbf{p}} = \\sigma(\\mathbf{X} \\cdot \\mathbf{\\theta})$\n",
    "\n",
    "That's all we need to build the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs + 1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n_inputs + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "logits = tf.matmul(X, theta, name=\"logits\")\n",
    "y_proba = 1 / (1 + tf.exp(-logits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, TensorFlow has a nice function `tf.sigmoid()` that we can use to simplify the last line of the previous code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_proba = tf.sigmoid(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw in chapter 4, the log loss is a good cost function to use for Logistic Regression:\n",
    "\n",
    "$J(\\mathbf{\\theta}) = -\\dfrac{1}{m} \\sum\\limits_{i=1}^{m}{\\left[ y^{(i)} log\\left(\\hat{p}^{(i)}\\right) + (1 - y^{(i)}) log\\left(1 - \\hat{p}^{(i)}\\right)\\right]}$\n",
    "\n",
    "One option is to implement it ourselves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epsilon = 1e-7  # to avoid an overflow when computing the log\n",
    "loss = -tf.reduce_mean(y * tf.log(y_proba + epsilon) + (1 - y) * tf.log(1 - y_proba + epsilon))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we might as well use TensorFlow's `tf.losses.log_loss()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = tf.losses.log_loss(y, y_proba)  # uses epsilon = 1e-7 by default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest is pretty standard: let's create the optimizer and tell it to minimize the cost function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All we need now (in this minimal version) is the variable initializer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we are ready to train the model and use it for predictions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's really nothing special about this code, it's virtually the same as the one we used earlier for Linear Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tLoss: 0.792602\n",
      "Epoch: 100 \tLoss: 0.343463\n",
      "Epoch: 200 \tLoss: 0.30754\n",
      "Epoch: 300 \tLoss: 0.292889\n",
      "Epoch: 400 \tLoss: 0.285336\n",
      "Epoch: 500 \tLoss: 0.280478\n",
      "Epoch: 600 \tLoss: 0.278083\n",
      "Epoch: 700 \tLoss: 0.276154\n",
      "Epoch: 800 \tLoss: 0.27552\n",
      "Epoch: 900 \tLoss: 0.274912\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1000\n",
    "batch_size = 50\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = random_batch(X_train, y_train, batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        loss_val = loss.eval({X: X_test, y: y_test})\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch:\", epoch, \"\\tLoss:\", loss_val)\n",
    "\n",
    "    y_proba_val = y_proba.eval(feed_dict={X: X_test, y: y_test})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: we don't use the epoch number when generating batches, so we could just have a single `for` loop rather than 2 nested `for` loops, but it's convenient to think of training time in terms of number of epochs (i.e., roughly the number of times the algorithm went through the training set)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each instance in the test set, `y_proba_val` contains the estimated probability that it belongs to the positive class, according to the model. For example, here are the first 5 estimated probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.54895616],\n",
       "       [ 0.70724374],\n",
       "       [ 0.51900256],\n",
       "       [ 0.9911136 ],\n",
       "       [ 0.50859052]], dtype=float32)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_proba_val[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To classify each instance, we can go for maximum likelihood: classify as positive any instance whose estimated probability is greater or equal to 0.5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True]], dtype=bool)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = (y_proba_val >= 0.5)\n",
    "y_pred[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the use case, you may want to choose a different threshold than 0.5: make it higher if you want high precision (but lower recall), and make it lower if you want high recall (but lower precision). See chapter 3 for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the model's precision and recall:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86274509803921573"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "precision_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88888888888888884"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot these predictions to see what they look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEFCAYAAAAIZiutAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nykEQBEHwQpSDIAiC4MX/B4BXNRO7H//+AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f96d0e11f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred_idx = y_pred.reshape(-1) # a 1D array rather than a column vector\n",
    "plt.plot(X_test[y_pred_idx, 1], X_test[y_pred_idx, 2], 'go', label=\"Positive\")\n",
    "plt.plot(X_test[~y_pred_idx, 1], X_test[~y_pred_idx, 2], 'r^', label=\"Negative\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that looks pretty bad, doesn't it? But let's not forget that the Logistic Regression model has a linear decision boundary, so this is actually close to the best we can do with this model (unless we add more features, as we will show in a second)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's start over, but this time we will add all the bells and whistles, as listed in the exercise:\n",
    "* Define the graph within a `logistic_regression()` function that can be reused easily.\n",
    "* Save checkpoints using a `Saver` at regular intervals during training, and save the final model at the end of training.\n",
    "* Restore the last checkpoint upon startup if training was interrupted.\n",
    "* Define the graph using nice scopes so the graph looks good in TensorBoard.\n",
    "* Add summaries to visualize the learning curves in TensorBoard.\n",
    "* Try tweaking some hyperparameters such as the learning rate or the mini-batch size and look at the shape of the learning curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start, we will add 4 more features to the inputs: ${x_1}^2$, ${x_2}^2$, ${x_1}^3$ and ${x_2}^3$. This was not part of the exercise, but it will demonstrate how adding features can improve the model. We will do this manually, but you could also add them using `sklearn.preprocessing.PolynomialFeatures`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_enhanced = np.c_[X_train,\n",
    "                         np.square(X_train[:, 1]),\n",
    "                         np.square(X_train[:, 2]),\n",
    "                         X_train[:, 1] ** 3,\n",
    "                         X_train[:, 2] ** 3]\n",
    "X_test_enhanced = np.c_[X_test,\n",
    "                        np.square(X_test[:, 1]),\n",
    "                        np.square(X_test[:, 2]),\n",
    "                        X_test[:, 1] ** 3,\n",
    "                        X_test[:, 2] ** 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what the \"enhanced\" training set looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.00000000e+00,  -5.14696757e-02,   4.44198631e-01,\n",
       "          2.64912752e-03,   1.97312424e-01,  -1.36349734e-04,\n",
       "          8.76459084e-02],\n",
       "       [  1.00000000e+00,   1.03201691e+00,  -4.19741157e-01,\n",
       "          1.06505890e+00,   1.76182639e-01,   1.09915879e+00,\n",
       "         -7.39511049e-02],\n",
       "       [  1.00000000e+00,   8.67891864e-01,  -2.54827114e-01,\n",
       "          7.53236288e-01,   6.49368582e-02,   6.53727646e-01,\n",
       "         -1.65476722e-02],\n",
       "       [  1.00000000e+00,   2.88850997e-01,  -4.48668621e-01,\n",
       "          8.34348982e-02,   2.01303531e-01,   2.41002535e-02,\n",
       "         -9.03185778e-02],\n",
       "       [  1.00000000e+00,  -8.33439108e-01,   5.35056649e-01,\n",
       "          6.94620746e-01,   2.86285618e-01,  -5.78924095e-01,\n",
       "          1.53179024e-01]])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_enhanced[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, next let's reset the default graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define the `logistic_regression()` function to create the graph. We will leave out the definition of the inputs `X` and the targets `y`. We could include them here, but leaving them out will make it easier to use this function in a wide range of use cases (e.g. perhaps we will want to add some preprocessing steps for the inputs before we feed them to the Logistic Regression model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_regression(X, y, initializer=None, seed=42, learning_rate=0.01):\n",
    "    n_inputs_including_bias = int(X.get_shape()[1])\n",
    "    with tf.name_scope(\"logistic_regression\"):\n",
    "        with tf.name_scope(\"model\"):\n",
    "            if initializer is None:\n",
    "                initializer = tf.random_uniform([n_inputs_including_bias, 1], -1.0, 1.0, seed=seed)\n",
    "            theta = tf.Variable(initializer, name=\"theta\")\n",
    "            logits = tf.matmul(X, theta, name=\"logits\")\n",
    "            y_proba = tf.sigmoid(logits)\n",
    "        with tf.name_scope(\"train\"):\n",
    "            loss = tf.losses.log_loss(y, y_proba, scope=\"loss\")\n",
    "            optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "            training_op = optimizer.minimize(loss)\n",
    "            loss_summary = tf.summary.scalar('log_loss', loss)\n",
    "        with tf.name_scope(\"init\"):\n",
    "            init = tf.global_variables_initializer()\n",
    "        with tf.name_scope(\"save\"):\n",
    "            saver = tf.train.Saver()\n",
    "    return y_proba, loss, training_op, loss_summary, init, saver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a little function to get the name of the log directory to save the summaries for Tensorboard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def log_dir(prefix=\"\"):\n",
    "    now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "    root_logdir = \"tf_logs\"\n",
    "    if prefix:\n",
    "        prefix += \"-\"\n",
    "    name = prefix + \"run-\" + now\n",
    "    return \"{}/{}/\".format(root_logdir, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's create the graph, using the `logistic_regression()` function. We will also create the `FileWriter` to save the summaries to the log directory for Tensorboard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_inputs = 2 + 4\n",
    "logdir = log_dir(\"logreg\")\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs + 1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "\n",
    "y_proba, loss, training_op, loss_summary, init, saver = logistic_regression(X, y)\n",
    "\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At last we can train the model! We will start by checking whether a previous training session was interrupted, and if so we will load the checkpoint and continue training from the epoch number we saved. In this example we just save the epoch number to a separate file, but in chapter 11 we will see how to store the training step directly as part of the model, using a non-trainable variable called `global_step` that we pass to the optimizer's `minimize()` method.\n",
    "\n",
    "You can try interrupting training to verify that it does indeed restore the last checkpoint when you start it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tLoss: 0.629985\n",
      "Epoch: 500 \tLoss: 0.161224\n",
      "Epoch: 1000 \tLoss: 0.119032\n",
      "Epoch: 1500 \tLoss: 0.0973292\n",
      "Epoch: 2000 \tLoss: 0.0836979\n",
      "Epoch: 2500 \tLoss: 0.0743758\n",
      "Epoch: 3000 \tLoss: 0.0675021\n",
      "Epoch: 3500 \tLoss: 0.0622069\n",
      "Epoch: 4000 \tLoss: 0.0580268\n",
      "Epoch: 4500 \tLoss: 0.054563\n",
      "Epoch: 5000 \tLoss: 0.0517083\n",
      "Epoch: 5500 \tLoss: 0.0492377\n",
      "Epoch: 6000 \tLoss: 0.0471673\n",
      "Epoch: 6500 \tLoss: 0.0453766\n",
      "Epoch: 7000 \tLoss: 0.0438187\n",
      "Epoch: 7500 \tLoss: 0.0423742\n",
      "Epoch: 8000 \tLoss: 0.0410892\n",
      "Epoch: 8500 \tLoss: 0.0399709\n",
      "Epoch: 9000 \tLoss: 0.0389202\n",
      "Epoch: 9500 \tLoss: 0.0380107\n",
      "Epoch: 10000 \tLoss: 0.0371557\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10001\n",
    "batch_size = 50\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "checkpoint_path = \"/tmp/my_logreg_model.ckpt\"\n",
    "checkpoint_epoch_path = checkpoint_path + \".epoch\"\n",
    "final_model_path = \"./my_logreg_model\"\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    if os.path.isfile(checkpoint_epoch_path):\n",
    "        # if the checkpoint file exists, restore the model and load the epoch number\n",
    "        with open(checkpoint_epoch_path, \"rb\") as f:\n",
    "            start_epoch = int(f.read())\n",
    "        print(\"Training was interrupted. Continuing at epoch\", start_epoch)\n",
    "        saver.restore(sess, checkpoint_path)\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "        sess.run(init)\n",
    "\n",
    "    for epoch in range(start_epoch, n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = random_batch(X_train_enhanced, y_train, batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        loss_val, summary_str = sess.run([loss, loss_summary], feed_dict={X: X_test_enhanced, y: y_test})\n",
    "        file_writer.add_summary(summary_str, epoch)\n",
    "        if epoch % 500 == 0:\n",
    "            print(\"Epoch:\", epoch, \"\\tLoss:\", loss_val)\n",
    "            saver.save(sess, checkpoint_path)\n",
    "            with open(checkpoint_epoch_path, \"wb\") as f:\n",
    "                f.write(b\"%d\" % (epoch + 1))\n",
    "\n",
    "    saver.save(sess, final_model_path)\n",
    "    y_proba_val = y_proba.eval(feed_dict={X: X_test_enhanced, y: y_test})\n",
    "    os.remove(checkpoint_epoch_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we can make predictions by just classifying as positive all the instances whose estimated probability is greater or equal to 0.5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = (y_proba_val >= 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97979797979797978"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97979797979797978"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEFCAYAAAAIZiutAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f96d29e95f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred_idx = y_pred.reshape(-1) # a 1D array rather than a column vector\n",
    "plt.plot(X_test[y_pred_idx, 1], X_test[y_pred_idx, 2], 'go', label=\"Positive\")\n",
    "plt.plot(X_test[~y_pred_idx, 1], X_test[~y_pred_idx, 2], 'r^', label=\"Negative\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that's much, much better! Apparently the new features really helped a lot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try starting the tensorboard server, find the latest run and look at the learning curve (i.e., how the loss evaluated on the test set evolves as a function of the epoch number):\n",
    "\n",
    "```\n",
    "$ tensorboard --logdir=tf_logs\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can play around with the hyperparameters (e.g. the `batch_size` or the `learning_rate`) and run training again and again, comparing the learning curves. You can even automate this process by implementing grid search or randomized search. Below is a simple implementation of a randomized search on both the batch size and the learning rate. For the sake of simplicity, the checkpoint mechanism was removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "  logdir: tf_logs/logreg-run-20170606195328/\n",
      "  batch size: 19\n",
      "  learning_rate: 0.00443037524522\n",
      "  training: .....................\n",
      "  precision: 0.979797979798\n",
      "  recall: 0.979797979798\n",
      "Iteration 1\n",
      "  logdir: tf_logs/logreg-run-20170606195605/\n",
      "  batch size: 80\n",
      "  learning_rate: 0.00178264971514\n",
      "  training: .....................\n",
      "  precision: 0.969696969697\n",
      "  recall: 0.969696969697\n",
      "Iteration 2\n",
      "  logdir: tf_logs/logreg-run-20170606195646/\n",
      "  batch size: 73\n",
      "  learning_rate: 0.00203228544324\n",
      "  training: .....................\n",
      "  precision: 0.969696969697\n",
      "  recall: 0.969696969697\n",
      "Iteration 3\n",
      "  logdir: tf_logs/logreg-run-20170606195730/\n",
      "  batch size: 6\n",
      "  learning_rate: 0.00449152382514\n",
      "  training: .....................\n",
      "  precision: 0.980198019802\n",
      "  recall: 1.0\n",
      "Iteration 4\n",
      "  logdir: tf_logs/logreg-run-20170606200523/\n",
      "  batch size: 24\n",
      "  learning_rate: 0.0796323472178\n",
      "  training: .....................\n",
      "  precision: 0.980198019802\n",
      "  recall: 1.0\n",
      "Iteration 5\n",
      "  logdir: tf_logs/logreg-run-20170606200726/\n",
      "  batch size: 75\n",
      "  learning_rate: 0.000463425058329\n",
      "  training: .....................\n",
      "  precision: 0.912621359223\n",
      "  recall: 0.949494949495\n",
      "Iteration 6\n",
      "  logdir: tf_logs/logreg-run-20170606200810/\n",
      "  batch size: 86\n",
      "  learning_rate: 0.0477068184194\n",
      "  training: .....................\n",
      "  precision: 0.98\n",
      "  recall: 0.989898989899\n",
      "Iteration 7\n",
      "  logdir: tf_logs/logreg-run-20170606200851/\n",
      "  batch size: 87\n",
      "  learning_rate: 0.000169404470952\n",
      "  training: .....................\n",
      "  precision: 0.888888888889\n",
      "  recall: 0.808080808081\n",
      "Iteration 8\n",
      "  logdir: tf_logs/logreg-run-20170606200932/\n",
      "  batch size: 61\n",
      "  learning_rate: 0.0417146119941\n",
      "  training: .....................\n",
      "  precision: 0.980198019802\n",
      "  recall: 1.0\n",
      "Iteration 9\n",
      "  logdir: tf_logs/logreg-run-20170606201026/\n",
      "  batch size: 92\n",
      "  learning_rate: 0.000107429229684\n",
      "  training: .....................\n",
      "  precision: 0.882352941176\n",
      "  recall: 0.757575757576\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import reciprocal\n",
    "\n",
    "n_search_iterations = 10\n",
    "\n",
    "for search_iteration in range(n_search_iterations):\n",
    "    batch_size = np.random.randint(1, 100)\n",
    "    learning_rate = reciprocal(0.0001, 0.1).rvs(random_state=search_iteration)\n",
    "\n",
    "    n_inputs = 2 + 4\n",
    "    logdir = log_dir(\"logreg\")\n",
    "    \n",
    "    print(\"Iteration\", search_iteration)\n",
    "    print(\"  logdir:\", logdir)\n",
    "    print(\"  batch size:\", batch_size)\n",
    "    print(\"  learning_rate:\", learning_rate)\n",
    "    print(\"  training: \", end=\"\")\n",
    "\n",
    "    reset_graph()\n",
    "\n",
    "    X = tf.placeholder(tf.float32, shape=(None, n_inputs + 1), name=\"X\")\n",
    "    y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "\n",
    "    y_proba, loss, training_op, loss_summary, init, saver = logistic_regression(\n",
    "        X, y, learning_rate=learning_rate)\n",
    "\n",
    "    file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "\n",
    "    n_epochs = 10001\n",
    "    n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "    final_model_path = \"./my_logreg_model_%d\" % search_iteration\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            for batch_index in range(n_batches):\n",
    "                X_batch, y_batch = random_batch(X_train_enhanced, y_train, batch_size)\n",
    "                sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            loss_val, summary_str = sess.run([loss, loss_summary], feed_dict={X: X_test_enhanced, y: y_test})\n",
    "            file_writer.add_summary(summary_str, epoch)\n",
    "            if epoch % 500 == 0:\n",
    "                print(\".\", end=\"\")\n",
    "\n",
    "        saver.save(sess, final_model_path)\n",
    "\n",
    "        print()\n",
    "        y_proba_val = y_proba.eval(feed_dict={X: X_test_enhanced, y: y_test})\n",
    "        y_pred = (y_proba_val >= 0.5)\n",
    "        \n",
    "        print(\"  precision:\", precision_score(y_test, y_pred))\n",
    "        print(\"  recall:\", recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `reciprocal()` function from SciPy's `stats` module returns a random distribution that is commonly used when you have no idea of the optimal scale of a hyperparameter. See the exercise solutions for chapter 2 for more details. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "nav_menu": {
   "height": "603px",
   "width": "616px"
  },
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
