* Features:
** train word embedding
** visualize word embedding in tensorboard
** preprocess wikipedia dump
** dump pages of wikipedia to mongodb
** retrieve content from mongodb and train word embedding model
* File description
| file_name                     | function                                              | log     |
| similar_words_gensim.py       | word embedding via gensim                             | phrases |
| train_word2vec_with_gensim.py | en wiki plain files to model                          |         |
| train_word2vec_txt.py         | train txt files to model                              |         |
| brown.model.bin               | nltk model                                            |         |
| convert_to_txt.py             | convert pdf/doc to txt                                |         |
| embedding_keras.py            | utilizing keras in word embedding training            |         |
| extract_wiki_page2xml.py      | extract wiki page from xml, print pages               |         |
| extract_wiki.py               | extract wiki page from xml in specific categories     | finance |
| jieba_cut.py                  | jieba cut example                                     |         |
| load_pre_trained_model.py     | load a pretrained model                               |         |
| parse_pages.py                | read wiki categories information                      |         |
| process_wiki.py               | process wiki xml with gensim corpus                   |         |
| text8                         |                                                       |         |
| text8.zip                     |                                                       |         |
| training.log                  |                                                       |         |
| train_nltk_word2vec_model.py  | train DJA Reddit news                                 |         |
| train_word2vec_model.py       | en wiki text to model and vector                      |         |
| wiki.en.text                  |                                                       |         |
| word2vec_basic.py             | using tensorflow to build word embedding from scratch |         |
| word2vec.py                   | Multi-threaded word2vec mini-batched skip-gram model  |         |
|                               |                                                       |         |
* Visualize embedding in tensorboard projector:
- how to visualize embedding at tensorboard:
open [[https://projector.tensorflow.org][tensorboard]], click load data at the left panel, step 1: load a tsv file of vectors, choose [[file:./enwiki_economy_pages_only/all_vocab/tensor.tsv]], step 2: load a tsv file of metadata, choose [[file:./enwiki_economy_pages_only/all_vocab/metadata.tsv]].

*you can also look up similar words at this tool at the right panel.*
- compare different model training from a larger corpus:
step 1: choose [[file:./finance_level_5/all_vocab/tensor.tsv]], step 2: choose [[file:./finance_level_5/all_vocab/metadata.tsv]].
- only visualize some finance vocabulary in tensorboard:
step 1: choose [[file:./finance_level_5/part_vocab/tensor.tsv]], step 2: choose [[file:./finance_level_5/part_vocab/metadata.tsv]].

* result:
** - intersection of two words:
#+BEGIN_SRC python
In [166]: ('intersection of: ', ('gdp', 'gnp'))
set([u'estimates', u'cbo', u'economies', u'relative', u'surpluses', u'inequality', u'gross', u'expenditures', u'ratio', u'consumption', u'incomes', u'projected', u'forecast', u'nominal', u'deficit', u'gdps', u'economy', u'coefficient', u'exports', u'gini', u'outlays', u'expenditure', u'cpi', u'gni', u'deficits', u'capita', u'growth', u'output', u'revenues'])
('intersection of: ', ('interest', 'debt'))
set([u'liabilities', u'bondholders', u'loan', u'balances', u'borrowings', u'mbs', u'creditor', u'mortgage', u'indebtedness', u'debts', u'unsecured', u'collateral', u'repayments', u'repayment', u'bonds', u'borrower', u'loans', u'borrowers', u'refinancing', u'default', u'borrowing', u'mortgages', u'defaults'])
('intersection of: ', ('finance', 'bank'))
set([u'banking'])
('intersection of: ', ('google', 'yahoo'))
set([u'twitter', u'skype', u'linkedin', u'tumblr', u'geocities', u'aol', u'gmail', u'spotify', u'pinterest', u'adwords', u'myspace', u'airbnb', u'foursquare', u'wordpress', u'doubleclick', u'adsense', u'whatsapp', u'flickr', u'baidu', u'hotmail', u'quora', u'facebook', u'snapchat', u'dropbox', u'netscape', u'bing', u'ebay'])

#+END_SRC
** syntactic/semantic NLP word tasks
- A - B = C - D?
#+BEGIN_SRC python
In [39]: model_wiki.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)
Out[40]: [(u'queen', 0.6665365099906921)]

In [102]: model_wiki.most_similar_cosmul(positive=['woman', 'king'], negative=['man'], topn=10)
Out[102]:
[(u'queen', 0.9017336368560791),
 (u'empress', 0.8621657490730286),
 (u'princess', 0.8518181443214417),
 (u'consort', 0.8241053819656372),
 (u'duchess', 0.8176387548446655),
 (u'catherine', 0.815154492855072),
 (u'regent', 0.8117901682853699),
 (u'dowager', 0.810664713382721),
 (u'electress', 0.8100847601890564),
 (u'throne', 0.8039511442184448)]
#+END_SRC

- A + B = ?
#+BEGIN_SRC python
In [96]: model_wiki.most_similar(['interest','cpi'])

Out[99]:
[(u'inflation', 0.5732479691505432),
 (u'apr', 0.5531845092773438),
 (u'rpi', 0.5135754942893982),
 (u'hicp', 0.5048391819000244),
 (u'calculation', 0.504572868347168),
 (u'libor', 0.5034551620483398),
 (u'percentage', 0.4969024658203125),
 (u'income', 0.4836910367012024),
 (u'coupon', 0.4777805507183075),
 (u'premiums', 0.471430242061615)]

#+END_SRC

- A, B, C, D, which is not qualified?
#+BEGIN_SRC python
In [103]: model_wiki.doesnt_match("cpi gdp interest lunch".split())
Out[107]: 'lunch'
#+END_SRC

- similarity of two words
#+BEGIN_SRC python
In [108]: model_wiki.similarity('cpi', 'gdp')
Out[112]: 0.50464583932408846

In [113]: model_wiki.similarity('cpi', 'inflation')
Out[116]: 0.4803993862786744
#+END_SRC
